{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 19:42:46,735 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Word2Vec training on BPE tokens...\n",
      "Parameters: vector_size=200, window=5, min_count=2, sg=1, negative=10, workers=4, epochs=15\n",
      "Reading corpus from: processed_darija_v2/darija_bpe_tokenized_for_w2v.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 19:43:02,371 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-05-06 19:43:30,695 : INFO : collected 49786 word types from a corpus of 112366414 raw words and 1 sentences\n",
      "2025-05-06 19:43:30,696 : INFO : Creating a fresh vocabulary\n",
      "2025-05-06 19:43:30,853 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 49668 unique words (99.76% of original 49786, drops 118)', 'datetime': '2025-05-06T19:43:30.853006', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-06 19:43:30,853 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 112366296 word corpus (100.00% of original 112366414, drops 118)', 'datetime': '2025-05-06T19:43:30.853829', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-06 19:43:31,074 : INFO : deleting the raw counts dictionary of 49786 items\n",
      "2025-05-06 19:43:31,085 : INFO : sample=0.001 downsamples 8 most-common words\n",
      "2025-05-06 19:43:31,086 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 111633584.54716942 word corpus (99.3%% of prior 112366296)', 'datetime': '2025-05-06T19:43:31.086910', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-06 19:43:31,482 : INFO : estimated required memory for 49668 words and 200 dimensions: 104302800 bytes\n",
      "2025-05-06 19:43:31,483 : INFO : resetting layer weights\n",
      "2025-05-06 19:43:31,547 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-05-06T19:43:31.547150', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'build_vocab'}\n",
      "2025-05-06 19:43:31,547 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 49668 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=10 window=5 shrink_windows=True', 'datetime': '2025-05-06T19:43:31.547952', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2025-05-06 19:43:47,317 : INFO : EPOCH 0 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:43:49,847 : INFO : EPOCH 0 - PROGRESS: at 100.00% examples, 547 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:43:49,847 : INFO : EPOCH 0: training on 112366414 raw words (10000 effective words) took 18.3s, 547 effective words/s\n",
      "2025-05-06 19:44:04,052 : INFO : EPOCH 1 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:44:06,646 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 595 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:44:06,647 : INFO : EPOCH 1: training on 112366414 raw words (10000 effective words) took 16.8s, 595 effective words/s\n",
      "2025-05-06 19:44:20,829 : INFO : EPOCH 2 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:44:23,399 : INFO : EPOCH 2 - PROGRESS: at 100.00% examples, 597 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:44:23,400 : INFO : EPOCH 2: training on 112366414 raw words (10000 effective words) took 16.8s, 597 effective words/s\n",
      "2025-05-06 19:44:37,510 : INFO : EPOCH 3 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:44:40,083 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 599 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:44:40,084 : INFO : EPOCH 3: training on 112366414 raw words (10000 effective words) took 16.7s, 599 effective words/s\n",
      "2025-05-06 19:44:54,243 : INFO : EPOCH 4 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:44:56,715 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 601 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:44:56,716 : INFO : EPOCH 4: training on 112366414 raw words (10000 effective words) took 16.6s, 601 effective words/s\n",
      "2025-05-06 19:45:10,640 : INFO : EPOCH 5 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:45:13,207 : INFO : EPOCH 5 - PROGRESS: at 100.00% examples, 606 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:45:13,208 : INFO : EPOCH 5: training on 112366414 raw words (10000 effective words) took 16.5s, 606 effective words/s\n",
      "2025-05-06 19:45:26,927 : INFO : EPOCH 6 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:45:29,473 : INFO : EPOCH 6 - PROGRESS: at 100.00% examples, 615 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:45:29,474 : INFO : EPOCH 6: training on 112366414 raw words (10000 effective words) took 16.3s, 615 effective words/s\n",
      "2025-05-06 19:45:43,191 : INFO : EPOCH 7 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:45:45,692 : INFO : EPOCH 7 - PROGRESS: at 100.00% examples, 617 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:45:45,693 : INFO : EPOCH 7: training on 112366414 raw words (10000 effective words) took 16.2s, 617 effective words/s\n",
      "2025-05-06 19:45:59,468 : INFO : EPOCH 8 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:46:02,017 : INFO : EPOCH 8 - PROGRESS: at 100.00% examples, 613 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:46:02,018 : INFO : EPOCH 8: training on 112366414 raw words (10000 effective words) took 16.3s, 613 effective words/s\n",
      "2025-05-06 19:46:15,873 : INFO : EPOCH 9 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:46:18,397 : INFO : EPOCH 9 - PROGRESS: at 100.00% examples, 611 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:46:18,398 : INFO : EPOCH 9: training on 112366414 raw words (10000 effective words) took 16.4s, 611 effective words/s\n",
      "2025-05-06 19:46:32,071 : INFO : EPOCH 10 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:46:34,573 : INFO : EPOCH 10 - PROGRESS: at 100.00% examples, 618 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:46:34,574 : INFO : EPOCH 10: training on 112366414 raw words (10000 effective words) took 16.2s, 618 effective words/s\n",
      "2025-05-06 19:46:48,331 : INFO : EPOCH 11 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:46:50,834 : INFO : EPOCH 11 - PROGRESS: at 100.00% examples, 615 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:46:50,834 : INFO : EPOCH 11: training on 112366414 raw words (10000 effective words) took 16.3s, 615 effective words/s\n",
      "2025-05-06 19:47:04,557 : INFO : EPOCH 12 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:47:07,068 : INFO : EPOCH 12 - PROGRESS: at 100.00% examples, 616 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:47:07,069 : INFO : EPOCH 12: training on 112366414 raw words (10000 effective words) took 16.2s, 616 effective words/s\n",
      "2025-05-06 19:47:20,898 : INFO : EPOCH 13 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:47:23,369 : INFO : EPOCH 13 - PROGRESS: at 100.00% examples, 614 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:47:23,370 : INFO : EPOCH 13: training on 112366414 raw words (10000 effective words) took 16.3s, 614 effective words/s\n",
      "2025-05-06 19:47:37,121 : INFO : EPOCH 14 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 19:47:39,612 : INFO : EPOCH 14 - PROGRESS: at 100.00% examples, 616 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 19:47:39,613 : INFO : EPOCH 14: training on 112366414 raw words (10000 effective words) took 16.2s, 616 effective words/s\n",
      "2025-05-06 19:47:39,614 : INFO : Word2Vec lifecycle event {'msg': 'training on 1685496210 raw words (150000 effective words) took 248.1s, 605 effective words/s', 'datetime': '2025-05-06T19:47:39.614141', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2025-05-06 19:47:39,614 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=49668, vector_size=200, alpha=0.025>', 'datetime': '2025-05-06T19:47:39.614726', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2025-05-06 19:47:39,616 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'darija_word2vec_bpe_sg_ns.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-05-06T19:47:39.616651', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'saving'}\n",
      "2025-05-06 19:47:39,617 : INFO : not storing attribute cum_table\n",
      "2025-05-06 19:47:39,757 : INFO : saved darija_word2vec_bpe_sg_ns.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model to: darija_word2vec_bpe_sg_ns.model\n",
      "Model saved successfully.\n",
      "\n",
      "Vocabulary size: 49668 unique BPE tokens (after applying min_count=2)\n",
      "\n",
      "Testing similarity for some example BPE tokens:\n",
      "Tokens most similar to 'غابه': [('تحنات', 0.27721866965293884), ('سابتيون', 0.2733318507671356), ('لحر', 0.26950228214263916), ('تورنا', 0.2614783048629761), ('دوير', 0.2575022578239441), ('واديي', 0.25627487897872925), ('شادويسك', 0.2517807185649872), ('سلاسيس', 0.24846717715263367), ('سوفيا', 0.2473089098930359), ('باللير', 0.24609500169754028)]\n",
      "Token 'الخدمة' not found in the vocabulary.\n",
      "Tokens most similar to 'كود': [('ئشه', 0.3136468529701233), ('بريسينت', 0.29064151644706726), ('كاراشي', 0.27888602018356323), ('خطر', 0.27258092164993286), ('تلوات', 0.26717591285705566), ('نعطيها', 0.26664310693740845), ('عتق', 0.2652406096458435), ('راشد', 0.2650744915008545), ('سدا', 0.2580049932003021), ('تاين', 0.25442740321159363)]\n",
      "Tokens most similar to 'سياره': [('قفط', 0.2844425141811371), ('ينكرا', 0.27507996559143066), ('بواس', 0.26721107959747314), ('سامسونك', 0.26526567339897156), ('جيفف', 0.26123130321502686), ('جاللايس', 0.25691041350364685), ('فرقه', 0.25664833188056946), ('تسمعي', 0.25331559777259827), ('فودنيه', 0.2532273232936859), ('كاتبغيه', 0.247445747256279)]\n",
      "Token 'سيارة' not found in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# --- Configuration ---\\\n",
    "# *** IMPORTANT: Point this to the output file from the BPE preprocessing step ***\n",
    "# input_corpus_file = 'processed_darija_v2/darija_processed_words_for_w2v.txt' # <<< Use the BPE tokenized file\n",
    "input_corpus_file = 'processed_darija_v2/darija_bpe_tokenized_for_w2v.txt'\n",
    "# Output file for the Word2Vec model\n",
    "output_model_file = 'darija_word2vec_bpe_sg_ns.model' # <<< Updated model name\n",
    "\n",
    "# Word2Vec Parameters (adjust as needed, especially min_count for subwords)\n",
    "vector_size = 200\n",
    "window = 5\n",
    "min_count = 2      # <<< You might lower min_count for subwords (e.g., 2-5)\n",
    "sg = 1\n",
    "negative = 10\n",
    "workers = multiprocessing.cpu_count()\n",
    "epochs = 15        # <<< Consider more epochs for subwords if corpus isn't huge\n",
    "\n",
    "# --- Setup Logging ---\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# --- Sentence Iterator (No change needed, reads space-separated tokens) ---\n",
    "class SentenceIterator:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def __iter__(self):\n",
    "        try:\n",
    "            with open(self.filepath, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    tokens = line.split() # Reads the space-separated BPE tokens\n",
    "                    if tokens:\n",
    "                        yield tokens\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Error: Input corpus file not found at {self.filepath}\")\n",
    "            raise\n",
    "\n",
    "# --- Train the Model ---\n",
    "print(f\"Starting Word2Vec training on BPE tokens...\")\n",
    "print(f\"Parameters: vector_size={vector_size}, window={window}, min_count={min_count}, sg={sg}, negative={negative}, workers={workers}, epochs={epochs}\")\n",
    "print(f\"Reading corpus from: {input_corpus_file}\")\n",
    "\n",
    "sentences = SentenceIterator(input_corpus_file)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    sg=sg,\n",
    "    negative=negative,\n",
    "    workers=workers,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Save the Model ---\n",
    "print(f\"Saving model to: {output_model_file}\")\n",
    "model.save(output_model_file)\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# --- Basic Model Testing (Optional) ---\n",
    "vocab_size = len(model.wv.key_to_index)\n",
    "print(f\"\\nVocabulary size: {vocab_size} unique BPE tokens (after applying min_count={min_count})\")\n",
    "\n",
    "# Example: Find similar BPE tokens\n",
    "# Note: Similarity will be between subwords now.\n",
    "# You might need to look for subwords of words you are interested in.\n",
    "# Example: If 'استغلال' was tokenized into 'است', 'غلال', you'd test those.\n",
    "example_tokens = ['غابه', 'الخدمة', 'كود', 'سياره', 'سيارة'] # Adjust based on actual BPE tokens\n",
    "\n",
    "print(\"\\nTesting similarity for some example BPE tokens:\")\n",
    "for token in example_tokens:\n",
    "    if token in model.wv: # Use __contains__ or check key_to_index\n",
    "        try:\n",
    "            similar_tokens = model.wv.most_similar(token, topn=10)\n",
    "            print(f\"Tokens most similar to '{token}': {similar_tokens}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not get similar tokens for '{token}': {e}\")\n",
    "    else:\n",
    "        print(f\"Token '{token}' not found in the vocabulary.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
