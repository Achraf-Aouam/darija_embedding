{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 11:20:24,891 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Word2Vec training on BPE tokens...\n",
      "Parameters: vector_size=200, window=5, min_count=2, sg=1, negative=3, workers=4, epochs=15\n",
      "Reading corpus from: processed_darija_v2/darija_bpe_tokenized_for_w2v.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 11:20:25,242 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-05-06 11:20:25,682 : INFO : collected 24936 word types from a corpus of 2167536 raw words and 1 sentences\n",
      "2025-05-06 11:20:25,682 : INFO : Creating a fresh vocabulary\n",
      "2025-05-06 11:20:25,757 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 24877 unique words (99.76% of original 24936, drops 59)', 'datetime': '2025-05-06T11:20:25.757783', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-06 11:20:25,758 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 2167477 word corpus (100.00% of original 2167536, drops 59)', 'datetime': '2025-05-06T11:20:25.758758', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-06 11:20:25,864 : INFO : deleting the raw counts dictionary of 24936 items\n",
      "2025-05-06 11:20:25,866 : INFO : sample=0.001 downsamples 16 most-common words\n",
      "2025-05-06 11:20:25,866 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2069483.2884765049 word corpus (95.5%% of prior 2167477)', 'datetime': '2025-05-06T11:20:25.866717', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-06 11:20:26,054 : INFO : estimated required memory for 24877 words and 200 dimensions: 52241700 bytes\n",
      "2025-05-06 11:20:26,055 : INFO : resetting layer weights\n",
      "2025-05-06 11:20:26,079 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-05-06T11:20:26.079833', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'build_vocab'}\n",
      "2025-05-06 11:20:26,080 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 24877 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=3 window=5 shrink_windows=True', 'datetime': '2025-05-06T11:20:26.080515', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2025-05-06 11:20:26,411 : INFO : EPOCH 0: training on 2167536 raw words (10000 effective words) took 0.3s, 30391 effective words/s\n",
      "2025-05-06 11:20:26,718 : INFO : EPOCH 1: training on 2167536 raw words (10000 effective words) took 0.3s, 32725 effective words/s\n",
      "2025-05-06 11:20:27,019 : INFO : EPOCH 2: training on 2167536 raw words (10000 effective words) took 0.3s, 33499 effective words/s\n",
      "2025-05-06 11:20:27,324 : INFO : EPOCH 3: training on 2167536 raw words (10000 effective words) took 0.3s, 32963 effective words/s\n",
      "2025-05-06 11:20:27,632 : INFO : EPOCH 4: training on 2167536 raw words (10000 effective words) took 0.3s, 32711 effective words/s\n",
      "2025-05-06 11:20:27,934 : INFO : EPOCH 5: training on 2167536 raw words (10000 effective words) took 0.3s, 33324 effective words/s\n",
      "2025-05-06 11:20:28,235 : INFO : EPOCH 6: training on 2167536 raw words (10000 effective words) took 0.3s, 33391 effective words/s\n",
      "2025-05-06 11:20:28,537 : INFO : EPOCH 7: training on 2167536 raw words (10000 effective words) took 0.3s, 33351 effective words/s\n",
      "2025-05-06 11:20:28,841 : INFO : EPOCH 8: training on 2167536 raw words (10000 effective words) took 0.3s, 33046 effective words/s\n",
      "2025-05-06 11:20:29,153 : INFO : EPOCH 9: training on 2167536 raw words (10000 effective words) took 0.3s, 32291 effective words/s\n",
      "2025-05-06 11:20:29,454 : INFO : EPOCH 10: training on 2167536 raw words (10000 effective words) took 0.3s, 33359 effective words/s\n",
      "2025-05-06 11:20:29,754 : INFO : EPOCH 11: training on 2167536 raw words (10000 effective words) took 0.3s, 33625 effective words/s\n",
      "2025-05-06 11:20:30,068 : INFO : EPOCH 12: training on 2167536 raw words (10000 effective words) took 0.3s, 31936 effective words/s\n",
      "2025-05-06 11:20:30,378 : INFO : EPOCH 13: training on 2167536 raw words (10000 effective words) took 0.3s, 32668 effective words/s\n",
      "2025-05-06 11:20:30,676 : INFO : EPOCH 14: training on 2167536 raw words (10000 effective words) took 0.3s, 33792 effective words/s\n",
      "2025-05-06 11:20:30,677 : INFO : Word2Vec lifecycle event {'msg': 'training on 32513040 raw words (150000 effective words) took 4.6s, 32634 effective words/s', 'datetime': '2025-05-06T11:20:30.677453', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2025-05-06 11:20:30,678 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=24877, vector_size=200, alpha=0.025>', 'datetime': '2025-05-06T11:20:30.677985', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2025-05-06 11:20:30,678 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'darija_word2vec_bpe_sg_ns.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-05-06T11:20:30.678826', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'saving'}\n",
      "2025-05-06 11:20:30,679 : INFO : not storing attribute cum_table\n",
      "2025-05-06 11:20:30,816 : INFO : saved darija_word2vec_bpe_sg_ns.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model to: darija_word2vec_bpe_sg_ns.model\n",
      "Model saved successfully.\n",
      "\n",
      "Vocabulary size: 24877 unique BPE tokens (after applying min_count=2)\n",
      "\n",
      "Testing similarity for some example BPE tokens:\n",
      "Tokens most similar to 'استغلال': [('الزراعيه', 0.9633772969245911), ('الاراضي', 0.9632007479667664), ('يد', 0.9529932737350464), ('سحاب', 0.9514985680580139), ('في', 0.950337290763855), ('نا', 0.9501468539237976), ('و', 0.9501280784606934), ('كل', 0.9474674463272095), ('ينا', 0.9467081427574158), ('علي', 0.9457512497901917)]\n",
      "Token 'bzaf' not found in the vocabulary.\n",
      "Tokens most similar to 'غلال': [('لقاسسيح', 0.28510451316833496), ('نستاهل', 0.25825485587120056), ('لمرا', 0.2532854974269867), ('كاتبغيني', 0.25202497839927673), ('تصوتوا', 0.248075932264328), ('فبيت', 0.2440420687198639), ('سيرهاني', 0.24261215329170227), ('تيره', 0.23698653280735016), ('غيبت', 0.23527854681015015), ('دالاع', 0.22881074249744415)]\n",
      "Token 'tomobil' not found in the vocabulary.\n",
      "Token 'rasso' not found in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# --- Configuration ---\\\n",
    "# *** IMPORTANT: Point this to the output file from the BPE preprocessing step ***\n",
    "input_corpus_file = 'processed_darija_v2/darija_bpe_tokenized_for_w2v.txt' # <<< Use the BPE tokenized file\n",
    "\n",
    "# Output file for the Word2Vec model\n",
    "output_model_file = 'darija_word2vec_bpe_sg_ns.model' # <<< Updated model name\n",
    "\n",
    "# Word2Vec Parameters (adjust as needed, especially min_count for subwords)\n",
    "vector_size = 200\n",
    "window = 5\n",
    "min_count = 2      # <<< You might lower min_count for subwords (e.g., 2-5)\n",
    "sg = 1\n",
    "negative = 3\n",
    "workers = multiprocessing.cpu_count()\n",
    "epochs = 15        # <<< Consider more epochs for subwords if corpus isn't huge\n",
    "\n",
    "# --- Setup Logging ---\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# --- Sentence Iterator (No change needed, reads space-separated tokens) ---\n",
    "class SentenceIterator:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def __iter__(self):\n",
    "        try:\n",
    "            with open(self.filepath, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    tokens = line.split() # Reads the space-separated BPE tokens\n",
    "                    if tokens:\n",
    "                        yield tokens\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Error: Input corpus file not found at {self.filepath}\")\n",
    "            raise\n",
    "\n",
    "# --- Train the Model ---\n",
    "print(f\"Starting Word2Vec training on BPE tokens...\")\n",
    "print(f\"Parameters: vector_size={vector_size}, window={window}, min_count={min_count}, sg={sg}, negative={negative}, workers={workers}, epochs={epochs}\")\n",
    "print(f\"Reading corpus from: {input_corpus_file}\")\n",
    "\n",
    "sentences = SentenceIterator(input_corpus_file)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    sg=sg,\n",
    "    negative=negative,\n",
    "    workers=workers,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Save the Model ---\n",
    "print(f\"Saving model to: {output_model_file}\")\n",
    "model.save(output_model_file)\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# --- Basic Model Testing (Optional) ---\n",
    "vocab_size = len(model.wv.key_to_index)\n",
    "print(f\"\\nVocabulary size: {vocab_size} unique BPE tokens (after applying min_count={min_count})\")\n",
    "\n",
    "# Example: Find similar BPE tokens\n",
    "# Note: Similarity will be between subwords now.\n",
    "# You might need to look for subwords of words you are interested in.\n",
    "# Example: If 'استغلال' was tokenized into 'است', 'غلال', you'd test those.\n",
    "example_tokens = ['استغلال', 'bzaf', 'غلال', 'tomobil', 'rasso'] # Adjust based on actual BPE tokens\n",
    "\n",
    "print(\"\\nTesting similarity for some example BPE tokens:\")\n",
    "for token in example_tokens:\n",
    "    if token in model.wv: # Use __contains__ or check key_to_index\n",
    "        try:\n",
    "            similar_tokens = model.wv.most_similar(token, topn=10)\n",
    "            print(f\"Tokens most similar to '{token}': {similar_tokens}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not get similar tokens for '{token}': {e}\")\n",
    "    else:\n",
    "        print(f\"Token '{token}' not found in the vocabulary.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
