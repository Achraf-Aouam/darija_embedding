{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 22:47:19,950 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Word2Vec training on BPE tokens...\n",
      "Parameters: vector_size=300, window=7, min_count=2, sg=1, negative=15, workers=32, epochs=15\n",
      "Reading corpus from: processed_darija_v2/darija_bpe_tokenized_for_w2v.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 22:47:30,555 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-05-06 22:47:48,960 : INFO : collected 34918 word types from a corpus of 117711433 raw words and 1 sentences\n",
      "2025-05-06 22:47:48,961 : INFO : Creating a fresh vocabulary\n",
      "2025-05-06 22:47:49,039 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 34865 unique words (99.85% of original 34918, drops 53)', 'datetime': '2025-05-06T22:47:49.039361', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-06 22:47:49,039 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 117711380 word corpus (100.00% of original 117711433, drops 53)', 'datetime': '2025-05-06T22:47:49.039890', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-06 22:47:49,149 : INFO : deleting the raw counts dictionary of 34918 items\n",
      "2025-05-06 22:47:49,152 : INFO : sample=0.001 downsamples 8 most-common words\n",
      "2025-05-06 22:47:49,153 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 116994972.81488231 word corpus (99.4%% of prior 117711380)', 'datetime': '2025-05-06T22:47:49.153201', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-06 22:47:49,341 : INFO : estimated required memory for 34865 words and 300 dimensions: 101108500 bytes\n",
      "2025-05-06 22:47:49,342 : INFO : resetting layer weights\n",
      "2025-05-06 22:47:49,378 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-05-06T22:47:49.378579', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'build_vocab'}\n",
      "2025-05-06 22:47:49,379 : INFO : Word2Vec lifecycle event {'msg': 'training model with 32 workers on 34865 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=15 window=7 shrink_windows=True', 'datetime': '2025-05-06T22:47:49.379103', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2025-05-06 22:47:59,369 : INFO : EPOCH 0 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:48:01,156 : INFO : EPOCH 0 - PROGRESS: at 100.00% examples, 850 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:48:01,156 : INFO : EPOCH 0: training on 117711433 raw words (10000 effective words) took 11.8s, 850 effective words/s\n",
      "2025-05-06 22:48:11,250 : INFO : EPOCH 1 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 33, out_qsize 0\n",
      "2025-05-06 22:48:12,980 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 846 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:48:12,981 : INFO : EPOCH 1: training on 117711433 raw words (10000 effective words) took 11.8s, 846 effective words/s\n",
      "2025-05-06 22:48:23,036 : INFO : EPOCH 2 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:48:24,775 : INFO : EPOCH 2 - PROGRESS: at 100.00% examples, 848 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:48:24,776 : INFO : EPOCH 2: training on 117711433 raw words (10000 effective words) took 11.8s, 848 effective words/s\n",
      "2025-05-06 22:48:34,831 : INFO : EPOCH 3 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:48:36,542 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 850 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:48:36,542 : INFO : EPOCH 3: training on 117711433 raw words (10000 effective words) took 11.8s, 850 effective words/s\n",
      "2025-05-06 22:48:46,595 : INFO : EPOCH 4 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:48:48,347 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 847 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:48:48,348 : INFO : EPOCH 4: training on 117711433 raw words (10000 effective words) took 11.8s, 847 effective words/s\n",
      "2025-05-06 22:48:58,447 : INFO : EPOCH 5 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:49:00,158 : INFO : EPOCH 5 - PROGRESS: at 100.00% examples, 847 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:49:00,159 : INFO : EPOCH 5: training on 117711433 raw words (10000 effective words) took 11.8s, 847 effective words/s\n",
      "2025-05-06 22:49:10,230 : INFO : EPOCH 6 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:49:11,938 : INFO : EPOCH 6 - PROGRESS: at 100.00% examples, 849 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:49:11,939 : INFO : EPOCH 6: training on 117711433 raw words (10000 effective words) took 11.8s, 849 effective words/s\n",
      "2025-05-06 22:49:21,970 : INFO : EPOCH 7 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:49:23,743 : INFO : EPOCH 7 - PROGRESS: at 100.00% examples, 848 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:49:23,743 : INFO : EPOCH 7: training on 117711433 raw words (10000 effective words) took 11.8s, 847 effective words/s\n",
      "2025-05-06 22:49:33,796 : INFO : EPOCH 8 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:49:35,556 : INFO : EPOCH 8 - PROGRESS: at 100.00% examples, 847 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:49:35,556 : INFO : EPOCH 8: training on 117711433 raw words (10000 effective words) took 11.8s, 847 effective words/s\n",
      "2025-05-06 22:49:45,643 : INFO : EPOCH 9 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:49:47,367 : INFO : EPOCH 9 - PROGRESS: at 100.00% examples, 847 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:49:47,368 : INFO : EPOCH 9: training on 117711433 raw words (10000 effective words) took 11.8s, 847 effective words/s\n",
      "2025-05-06 22:49:57,393 : INFO : EPOCH 10 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:49:59,156 : INFO : EPOCH 10 - PROGRESS: at 100.00% examples, 849 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:49:59,157 : INFO : EPOCH 10: training on 117711433 raw words (10000 effective words) took 11.8s, 849 effective words/s\n",
      "2025-05-06 22:50:09,185 : INFO : EPOCH 11 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:50:10,922 : INFO : EPOCH 11 - PROGRESS: at 100.00% examples, 850 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:50:10,922 : INFO : EPOCH 11: training on 117711433 raw words (10000 effective words) took 11.8s, 850 effective words/s\n",
      "2025-05-06 22:50:21,016 : INFO : EPOCH 12 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:50:22,742 : INFO : EPOCH 12 - PROGRESS: at 100.00% examples, 846 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:50:22,742 : INFO : EPOCH 12: training on 117711433 raw words (10000 effective words) took 11.8s, 846 effective words/s\n",
      "2025-05-06 22:50:32,791 : INFO : EPOCH 13 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:50:34,510 : INFO : EPOCH 13 - PROGRESS: at 100.00% examples, 850 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:50:34,511 : INFO : EPOCH 13: training on 117711433 raw words (10000 effective words) took 11.8s, 850 effective words/s\n",
      "2025-05-06 22:50:44,581 : INFO : EPOCH 14 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-06 22:50:46,301 : INFO : EPOCH 14 - PROGRESS: at 100.00% examples, 849 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-06 22:50:46,301 : INFO : EPOCH 14: training on 117711433 raw words (10000 effective words) took 11.8s, 848 effective words/s\n",
      "2025-05-06 22:50:46,302 : INFO : Word2Vec lifecycle event {'msg': 'training on 1765671495 raw words (150000 effective words) took 176.9s, 848 effective words/s', 'datetime': '2025-05-06T22:50:46.302319', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2025-05-06 22:50:46,302 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=34865, vector_size=300, alpha=0.025>', 'datetime': '2025-05-06T22:50:46.302657', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2025-05-06 22:50:46,303 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'darija_word2vec_bpe_sg_ns.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-05-06T22:50:46.303429', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'saving'}\n",
      "2025-05-06 22:50:46,303 : INFO : not storing attribute cum_table\n",
      "2025-05-06 22:50:46,409 : INFO : saved darija_word2vec_bpe_sg_ns.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model to: darija_word2vec_bpe_sg_ns.model\n",
      "Model saved successfully.\n",
      "\n",
      "Vocabulary size: 34865 unique BPE tokens (after applying min_count=2)\n",
      "\n",
      "Testing similarity for some example BPE tokens:\n",
      "Tokens most similar to 'غابه': [('مهرس', 0.24660369753837585), ('بونسيس', 0.21888893842697144), ('شاشينك', 0.21241189539432526), ('غيق', 0.21021458506584167), ('كتوره', 0.2086116373538971), ('بحا', 0.20843039453029633), ('مايي', 0.20755049586296082), ('دوب', 0.2046542912721634), ('رشا', 0.20246878266334534), ('سيروم', 0.20173980295658112)]\n",
      "Token 'الخدمة' not found in the vocabulary.\n",
      "Tokens most similar to 'كود': [('نهايه', 0.21241676807403564), ('فيرري', 0.20696209371089935), ('شديد', 0.1996350884437561), ('وناضت', 0.1964312493801117), ('كيزدح', 0.19475223124027252), ('اااار', 0.19466352462768555), ('غانس', 0.19454215466976166), ('يرين', 0.1923326849937439), ('فومينت', 0.18977361917495728), ('هيراك', 0.18859979510307312)]\n",
      "Tokens most similar to 'سياره': [('شيلو', 0.25746649503707886), ('لمري', 0.22476987540721893), ('كايتسنا', 0.22101640701293945), ('مجددا', 0.2204417884349823), ('بنسبه', 0.21800096333026886), ('سييم', 0.2137921005487442), ('هومانيت', 0.2095882147550583), ('ماكيس', 0.20618221163749695), ('كاناس', 0.20498111844062805), ('جببب', 0.20434854924678802)]\n",
      "Token 'سيارة' not found in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_corpus_file = 'processed_darija_v2/darija_bpe_tokenized_for_w2v.txt'\n",
    "\n",
    "output_model_file = 'darija_word2vec_bpe_sg_ns.model'\n",
    "\n",
    "\n",
    "vector_size = 300\n",
    "window = 7\n",
    "min_count = 2\n",
    "sg = 1\n",
    "negative = 15\n",
    "workers = multiprocessing.cpu_count()\n",
    "epochs = 15\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "class SentenceIterator:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def __iter__(self):\n",
    "        try:\n",
    "            with open(self.filepath, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    tokens = line.split()\n",
    "                    if tokens:\n",
    "                        yield tokens\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Error: Input corpus file not found at {self.filepath}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "print(f\"Starting Word2Vec training on BPE tokens...\")\n",
    "print(f\"Parameters: vector_size={vector_size}, window={window}, min_count={min_count}, sg={sg}, negative={negative}, workers={workers}, epochs={epochs}\")\n",
    "print(f\"Reading corpus from: {input_corpus_file}\")\n",
    "\n",
    "sentences = SentenceIterator(input_corpus_file)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    sg=sg,\n",
    "    negative=negative,\n",
    "    workers=workers,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "print(f\"Saving model to: {output_model_file}\")\n",
    "model.save(output_model_file)\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "\n",
    "vocab_size = len(model.wv.key_to_index)\n",
    "print(f\"\\nVocabulary size: {vocab_size} unique BPE tokens (after applying min_count={min_count})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "example_tokens = ['غابه', 'الخدمة', 'كود', 'سياره', 'سيارة']\n",
    "\n",
    "print(\"\\nTesting similarity for some example BPE tokens:\")\n",
    "for token in example_tokens:\n",
    "    if token in model.wv:\n",
    "        try:\n",
    "            similar_tokens = model.wv.most_similar(token, topn=10)\n",
    "            print(f\"Tokens most similar to '{token}': {similar_tokens}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not get similar tokens for '{token}': {e}\")\n",
    "    else:\n",
    "        print(f\"Token '{token}' not found in the vocabulary.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
