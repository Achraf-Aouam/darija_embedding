{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import glob\n",
    "import unicodedata\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_files(directory_path):\n",
    "    \"\"\"\n",
    "    Read all .txt files from the specified directory and concatenate them.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to the directory containing the .txt files\n",
    "        \n",
    "    Returns:\n",
    "        A string containing all the text data\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    file_paths = glob.glob(os.path.join(directory_path, \"*.txt\"))\n",
    "    \n",
    "    if not file_paths:\n",
    "        raise ValueError(f\"No .txt files found in {directory_path}\")\n",
    "    \n",
    "    print(f\"Found {len(file_paths)} text files.\")\n",
    "    \n",
    "    for file_path in tqdm(file_paths, desc=\"Reading files\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                all_text += text + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove noise like email addresses, URLs, file markup, etc.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove file markup like [[File:…]]\n",
    "    text = re.sub(r'\\[\\[File:[^\\]]*\\]\\]', '', text)\n",
    "    \n",
    "    # Remove other markups\n",
    "    text = re.sub(r'\\[\\[[^\\]]*\\]\\]', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove script-mismatches (e.g., \"100px\")\n",
    "    text = re.sub(r'\\d+px', '', text)\n",
    "    \n",
    "    # Remove numbers in brackets\n",
    "    text = re.sub(r'\\(\\d+\\)', '', text)\n",
    "    \n",
    "    # Keep only Arabic, Latin letters, numbers, and whitespace\n",
    "    # This preserves Arabic and Latin text while removing other unwanted characters\n",
    "    text = re.sub(r'[^\\p{Arabic}\\p{Latin}\\d\\s]', ' ', text, flags=re.UNICODE)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    \"\"\"\n",
    "    Normalize Arabic text by removing diacritics and unifying letter variants.\n",
    "    \n",
    "    Args:\n",
    "        text: Arabic text\n",
    "        \n",
    "    Returns:\n",
    "        Normalized Arabic text\n",
    "    \"\"\"\n",
    "    # Remove diacritics (tashkeel)\n",
    "    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n",
    "    # Remove tatweel (ـ) character used for justification\n",
    "    text = text.replace('\\u0640', '')\n",
    "    # Normalize alef variants to plain alef (ا)\n",
    "    # Normalize alef variants to plain alef (ا)\n",
    "    text = text.replace('أ', 'ا')\n",
    "    text = text.replace('إ', 'ا')\n",
    "    text = text.replace('آ', 'ا')\n",
    "    text = text.replace('ٱ', 'ا') # Alef Wasla\n",
    "    \n",
    "    # Normalize other common variants\n",
    "    text = text.replace('ة', 'ه')  # ta marbuta -> ha\n",
    "    text = text.replace('ى', 'ي')  # alef maksura -> ya\n",
    "    text = text.replace('گ', 'ك')  # variant kaf -> kaf\n",
    "    \n",
    "    \n",
    "    text = text.replace('ڤ', 'ف')  # variant fa -> fa (V)\n",
    "    text = text.replace('پ', 'ب')  # variant ba -> ba (P)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_arabic_script(text):\n",
    "    \"\"\"\n",
    "    Convert Arabizi (Latin script with numbers) and common Latin letters \n",
    "    used for Arabic back to Arabic script. Leaves existing Arabic script untouched.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text (potentially mixed script)\n",
    "        \n",
    "    Returns:\n",
    "        Text with Arabizi parts converted to Arabic script.\n",
    "    \"\"\"\n",
    "    # Ensure lowercase for consistent matching of Latin characters\n",
    "    text = text.lower() \n",
    "\n",
    "    # Define mapping: More specific first (digraphs, numbers)\n",
    "    # Note: This is a common but simplified mapping. Some letters have ambiguity (e.g., g, s, d, t).\n",
    "    arabizi_map = {\n",
    "        # Digraphs (MUST be processed before single letters)\n",
    "        'sh': 'ش', 'kh': 'خ', 'gh': 'غ', 'dh': 'ذ', 'th': 'ث',\n",
    "        # Numbers used as letters\n",
    "        '3': 'ع', '7': 'ح', '9': 'ق', '2': 'ء', \n",
    "        '5': 'خ', # Common alternative/typo for kh\n",
    "        '8': 'غ', # Sometimes used for gh\n",
    "        # Single letters (common representations)\n",
    "        'a': 'ا', 'b': 'ب', 't': 'ت', 'j': 'ج', 'd': 'د',\n",
    "        'r': 'ر', 'z': 'ز', 's': 'س', 'f': 'ف', 'k': 'ك',\n",
    "        'l': 'ل', 'm': 'م', 'n': 'ن', 'h': 'ه', 'w': 'و', 'y': 'ي',\n",
    "        # Ambiguous mappings or less frequent (handle carefully)\n",
    "        'g': 'ك', # Often represents ك or ق in Darija, using ك as a default\n",
    "        'p': 'ب', # Map P to Ba\n",
    "        'v': 'ف', # Map V to Fa\n",
    "        # Vowels: 'a', 'w', 'y' mapped above. 'e', 'i', 'o', 'u' are tricky.\n",
    "        # They often represent removed diacritics or are part of foreign words.\n",
    "        # Mapping them can garble foreign words. Let's map 'i'->'ي' and leave others for now.\n",
    "        'i': 'ي', \n",
    "        # 'e': 'ي', # Optional: map 'e' to 'ya'\n",
    "        # 'o': 'و', # Optional: map 'o' to 'waw'\n",
    "        # 'u': 'و', # Optional: map 'u' to 'waw'\n",
    "    }\n",
    "\n",
    "    # Apply replacements iteratively:\n",
    "    # 1. Digraphs first to avoid partial replacements (e.g., 'h' in 'sh')\n",
    "    text = re.sub(r'sh', arabizi_map['sh'], text)\n",
    "    text = re.sub(r'kh', arabizi_map['kh'], text)\n",
    "    text = re.sub(r'gh', arabizi_map['gh'], text)\n",
    "    text = re.sub(r'dh', arabizi_map['dh'], text)\n",
    "    text = re.sub(r'th', arabizi_map['th'], text)\n",
    "\n",
    "    # 2. Replace numbers\n",
    "    text = text.replace('3', arabizi_map['3'])\n",
    "    text = text.replace('7', arabizi_map['7'])\n",
    "    text = text.replace('9', arabizi_map['9'])\n",
    "    text = text.replace('2', arabizi_map['2'])\n",
    "    text = text.replace('5', arabizi_map['5'])\n",
    "    text = text.replace('8', arabizi_map['8'])\n",
    "\n",
    "    # 3. Replace single letters (iterate through remaining map)\n",
    "    # Ensure we don't re-process parts of digraphs incorrectly\n",
    "    single_letter_map = {k: v for k, v in arabizi_map.items() if len(k) == 1 and k not in '379258'}\n",
    "    for lat, ar in single_letter_map.items():\n",
    "         text = text.replace(lat, ar) # Simple replacement\n",
    "\n",
    "    # Note: This process might transliterate actual Latin words (French/English).\n",
    "    # Example: \"gmail\" might become \"كمايل\" depending on the map.\n",
    "    # This is a trade-off for converting Arabizi written only with letters.\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text on whitespace. Preserves words.\n",
    "    (No changes from original, but now operates on Arabic script)\n",
    "    \"\"\"\n",
    "    # Split based on one or more whitespace characters\n",
    "    tokens = re.split(r'\\s+', text)\n",
    "    # Filter out empty strings that might result from multiple spaces\n",
    "    tokens = [token for token in tokens if token]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_darija_corpus_to_arabic(directory_path, sample_size=None):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for Darija corpus, normalizing towards Arabic script.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing .txt files\n",
    "        sample_size: If specified, only process this many characters (for testing)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (raw_text_sample, final_arabic_text, tokens, corpus_stats)\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Reading corpus files...\")\n",
    "    raw_text_full = read_corpus_files(directory_path) # Read full first for stats\n",
    "\n",
    "    raw_char_count = len(raw_text_full)\n",
    "    # Estimate word count on raw text before sampling\n",
    "    word_count_approx = len(re.findall(r'\\b\\w+\\b', raw_text_full))\n",
    "    print(f\"\\nRaw corpus statistics (full):\")\n",
    "    print(f\"- Total characters: {raw_char_count}\")\n",
    "    print(f\"- Total words (approx): {word_count_approx}\")\n",
    "\n",
    "    # Apply sampling *after* getting full stats if needed\n",
    "    if sample_size:\n",
    "        print(f\"\\nTaking a sample of {sample_size} characters for processing...\")\n",
    "        raw_text_processed = raw_text_full[:sample_size]\n",
    "    else:\n",
    "        raw_text_processed = raw_text_full\n",
    "   \n",
    "    \n",
    "    print(\"\\nStep 2: Cleaning text (removing noise)...\")\n",
    "    cleaned_text = clean_text(raw_text_processed)\n",
    "    \n",
    "    print(\"\\nStep 3: Normalizing existing Arabic script (diacritics, variants)...\")\n",
    "    # Apply this first to standardize any existing Arabic text\n",
    "    normalized_arabic_part = normalize_arabic(cleaned_text) \n",
    "    \n",
    "    print(\"\\nStep 4: Converting Arabizi/Latin to Arabic script...\")\n",
    "    # Now convert Latin/Arabizi numbers/letters to their Arabic equivalents\n",
    "    final_text = normalize_to_arabic_script(normalized_arabic_part)\n",
    "    \n",
    "    # Optional: Apply Arabic normalization again to catch any artifacts from conversion\n",
    "    # final_text = normalize_arabic(final_text) \n",
    "    # Usually not necessary if normalize_to_arabic_script produces clean Arabic\n",
    "\n",
    "    print(\"\\nStep 5: Tokenizing text...\")\n",
    "    tokens = tokenize_text(final_text)\n",
    "    \n",
    "    # Calculate some corpus statistics on the final processed text\n",
    "    corpus_stats = {\n",
    "        \"raw_chars_processed\": len(raw_text_processed),\n",
    "        \"final_chars\": len(final_text),\n",
    "        \"total_tokens\": len(tokens),\n",
    "        \"unique_tokens\": len(set(tokens)),\n",
    "        \"avg_token_length\": sum(len(t) for t in tokens) / len(tokens) if tokens else 0\n",
    "    }\n",
    "    \n",
    "    print(\"\\nFinal corpus statistics (Arabic normalized):\")\n",
    "    print(f\"- Total characters processed: {corpus_stats['final_chars']}\")\n",
    "    print(f\"- Total tokens: {corpus_stats['total_tokens']}\")\n",
    "    print(f\"- Unique tokens: {corpus_stats['unique_tokens']}\")\n",
    "    print(f\"- Average token length: {corpus_stats['avg_token_length']:.2f}\")\n",
    "    \n",
    "    return raw_text_processed, final_text, tokens, corpus_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_corpus(tokens, top_n=30):\n",
    "    \"\"\"\n",
    "    Analyze the corpus and create visualizations.\n",
    "    (No changes from original, but expects Arabic tokens now)\n",
    "    \"\"\"\n",
    "    # Calculate token frequencies\n",
    "    token_freq = {}\n",
    "    for token in tokens:\n",
    "        token_freq[token] = token_freq.get(token, 0) + 1 # More concise way\n",
    "    \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(token_freq.items(), columns=['token', 'frequency'])\n",
    "    df = df.sort_values('frequency', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "    # Add token length\n",
    "    df['length'] = df['token'].apply(len)\n",
    "    \n",
    "    # Calculate relative frequency\n",
    "    total_tokens = len(tokens)\n",
    "    df['rel_frequency'] = df['frequency'] / total_tokens\n",
    "    \n",
    "    # Plot top token frequencies\n",
    "    # Ensure matplotlib can display Arabic characters\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans' # Or another font supporting Arabic like 'Arial'\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # For Arabic text, ensure the font used supports it\n",
    "    # Using the top_n tokens directly might be better than seaborn index mapping\n",
    "    top_tokens = df['token'].head(top_n)\n",
    "    top_freqs = df['frequency'].head(top_n)\n",
    "    plt.bar(top_tokens, top_freqs) \n",
    "    plt.title(f'Top {top_n} Token Frequencies (Arabic)')\n",
    "    plt.xticks(rotation=65, ha='right', fontsize=10) # Adjust rotation/size as needed\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Token')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot token length distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df['length'], bins=max(1, min(df['length'].max(), 50))) # Adjust bins\n",
    "    plt.title('Token Length Distribution (Arabic)')\n",
    "    plt.xlabel('Token Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the corpus (use a sample size for testing)\n",
    "# Set sample_size=None to process the entire corpus\n",
    "raw_text_sample, processed_arabic_text, arabic_tokens, stats = preprocess_darija_corpus_to_arabic(\n",
    "    corpus_dir, \n",
    "    sample_size=None  # Adjust as needed\n",
    ")\n",
    "\n",
    "# --- Display Sample Output ---\n",
    "print(\"\\n\" + \"=\"*20 + \" Sample of Raw Text \" + \"=\"*20)\n",
    "print(raw_text_sample[:500]) # Show a bit more raw text\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" Sample of Processed Arabic Text \" + \"=\"*20)\n",
    "print(processed_arabic_text[:500])\n",
    "\n",
    "# --- Analyze the Arabic Tokens ---\n",
    "print(\"\\n\" + \"=\"*20 + \" Corpus Analysis (Arabic Tokens) \" + \"=\"*20)\n",
    "token_df_arabic = analyze_corpus(arabic_tokens, top_n=30)\n",
    "\n",
    "# Display top tokens\n",
    "print(\"\\nTop 30 tokens (Arabic):\")\n",
    "print(token_df_arabic.head(30))\n",
    "\n",
    "# --- Save the Results ---\n",
    "# Save the processed Arabic text to a file\n",
    "output_file_arabic = 'preprocessed_darija_corpus_arabic.txt'\n",
    "with open(output_file_arabic, 'w', encoding='utf-8') as f:\n",
    "    f.write(processed_arabic_text)\n",
    "print(f\"\\nSaved preprocessed Arabic corpus to {output_file_arabic}\")\n",
    "\n",
    "# Save the token list (one token per line) for word2vec training\n",
    "tokens_file_arabic = 'darija_tokens_arabic.txt'\n",
    "with open(tokens_file_arabic, 'w', encoding='utf-8') as f:\n",
    "    for token in arabic_tokens:\n",
    "        f.write(token + '\\n')\n",
    "print(f\"Saved {len(arabic_tokens)} Arabic tokens to {tokens_file_arabic}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
