{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:09:45,870 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Word2Vec training...\n",
      "Parameters: vector_size=100, window=5, min_count=8, sg=1, negative=5, workers=4, epochs=5\n",
      "Reading corpus from: /teamspace/studios/this_studio/preprocessed_darija_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:09:58,970 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-05-05 14:10:29,437 : INFO : collected 4975067 word types from a corpus of 101229938 raw words and 1 sentences\n",
      "2025-05-05 14:10:29,437 : INFO : Creating a fresh vocabulary\n",
      "2025-05-05 14:10:32,700 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=8 retains 485571 unique words (9.76% of original 4975067, drops 4489496)', 'datetime': '2025-05-05T14:10:32.700308', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-05 14:10:32,701 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 94248266 word corpus (93.10% of original 101229938, drops 6981672)', 'datetime': '2025-05-05T14:10:32.701352', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-05 14:10:35,173 : INFO : deleting the raw counts dictionary of 4975067 items\n",
      "2025-05-05 14:10:35,412 : INFO : sample=0.001 downsamples 24 most-common words\n",
      "2025-05-05 14:10:35,413 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 90121573.51234597 word corpus (95.6%% of prior 94248266)', 'datetime': '2025-05-05T14:10:35.413205', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2025-05-05 14:10:39,465 : INFO : estimated required memory for 485571 words and 100 dimensions: 631242300 bytes\n",
      "2025-05-05 14:10:39,466 : INFO : resetting layer weights\n",
      "2025-05-05 14:10:39,660 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-05-05T14:10:39.660345', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'build_vocab'}\n",
      "2025-05-05 14:10:39,661 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 485571 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-05-05T14:10:39.661172', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2025-05-05 14:10:48,172 : INFO : EPOCH 0 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-05 14:10:49,931 : INFO : EPOCH 0 - PROGRESS: at 100.00% examples, 974 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-05 14:10:49,932 : INFO : EPOCH 0: training on 101229938 raw words (10000 effective words) took 10.3s, 974 effective words/s\n",
      "2025-05-05 14:10:58,424 : INFO : EPOCH 1 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-05 14:11:00,017 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 992 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-05 14:11:00,019 : INFO : EPOCH 1: training on 101229938 raw words (10000 effective words) took 10.1s, 992 effective words/s\n",
      "2025-05-05 14:11:08,433 : INFO : EPOCH 2 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-05 14:11:10,005 : INFO : EPOCH 2 - PROGRESS: at 100.00% examples, 1002 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-05 14:11:10,007 : INFO : EPOCH 2: training on 101229938 raw words (10000 effective words) took 10.0s, 1001 effective words/s\n",
      "2025-05-05 14:11:18,433 : INFO : EPOCH 3 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-05 14:11:19,995 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 1001 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-05 14:11:19,996 : INFO : EPOCH 3: training on 101229938 raw words (10000 effective words) took 10.0s, 1001 effective words/s\n",
      "2025-05-05 14:11:28,468 : INFO : EPOCH 4 - PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0\n",
      "2025-05-05 14:11:29,921 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 1008 words/s, in_qsize 0, out_qsize 1\n",
      "2025-05-05 14:11:29,922 : INFO : EPOCH 4: training on 101229938 raw words (10000 effective words) took 9.9s, 1008 effective words/s\n",
      "2025-05-05 14:11:29,923 : INFO : Word2Vec lifecycle event {'msg': 'training on 506149690 raw words (50000 effective words) took 50.3s, 995 effective words/s', 'datetime': '2025-05-05T14:11:29.923495', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2025-05-05 14:11:29,924 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=485571, vector_size=100, alpha=0.025>', 'datetime': '2025-05-05T14:11:29.924891', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2025-05-05 14:11:30,106 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'darija_word2vec_sg_ns.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-05-05T14:11:30.106837', 'gensim': '4.3.3', 'python': '3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-1082-aws-x86_64-with-glibc2.31', 'event': 'saving'}\n",
      "2025-05-05 14:11:30,107 : INFO : storing np array 'vectors' to darija_word2vec_sg_ns.model.wv.vectors.npy\n",
      "2025-05-05 14:11:30,247 : INFO : storing np array 'syn1neg' to darija_word2vec_sg_ns.model.syn1neg.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model to: darija_word2vec_sg_ns.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:11:30,384 : INFO : not storing attribute cum_table\n",
      "2025-05-05 14:11:30,808 : INFO : saved darija_word2vec_sg_ns.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n",
      "\n",
      "Vocabulary size: 485571 unique tokens (after applying min_count=8)\n",
      "\n",
      "Testing similarity for some example words:\n",
      "Words most similar to 'ana': [('wana', 0.9797440767288208), ('allh', 0.974860668182373), ('raj3wn', 0.9737260937690735), ('mn', 0.9695346355438232), ('alyh', 0.9680131673812866)]\n",
      "Words most similar to 'bzaf': [('mn', 0.8952898979187012), ('dyal', 0.8940796256065369), ('alb7r', 0.8913525938987732), ('3ly', 0.8908437490463257), ('fy', 0.8885353207588196)]\n",
      "Words most similar to 'mzn': [('wanted', 0.4465177357196808), ('israelis', 0.4374731779098511), ('alzmrdytyn', 0.43694591522216797), ('wt7naa', 0.4257921278476715), ('sharly', 0.4121866524219513)]\n",
      "Words most similar to 'maroc': [('timei', 0.44330522418022156), ('httpstcolwsh7pznen', 0.4180850684642792), ('wnghwt', 0.4113446772098541), ('mextiby83', 0.4098399877548218), ('imaanne', 0.40979671478271484)]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "# Optional: If you want to use all available cores for faster training\n",
    "import multiprocessing\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file: Contains the preprocessed text (one sentence/document per line, tokens separated by space)\n",
    "# This file was generated by your previous preprocessing script.\n",
    "input_corpus_file = '/teamspace/studios/this_studio/preprocessed_darija_corpus.txt'\n",
    "\n",
    "# Output file: Where to save the trained Word2Vec model\n",
    "output_model_file = 'darija_word2vec_sg_ns.model'\n",
    "\n",
    "# Word2Vec Parameters (as requested)\n",
    "vector_size = 100  # Dimensionality of the word vectors (common: 100-300)\n",
    "window = 5         # Max distance between current and predicted word within a sentence\n",
    "min_count = 8      # Ignores all words with total frequency lower than this\n",
    "sg = 1             # Use Skip-Gram algorithm (0 for CBOW)\n",
    "negative = 5       # Number of negative samples (common: 5-20)\n",
    "workers = multiprocessing.cpu_count() # Number of worker threads to train the model (use all available cores)\n",
    "epochs = 5        # Number of iterations (epochs) over the corpus (common: 5-10)\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Gensim uses Python's logging module to report progress\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# --- Create Sentence Iterator ---\n",
    "# Word2Vec requires the input corpus to be an iterable of lists of tokens (sentences).\n",
    "# We create a simple iterator class that reads your preprocessed file line-by-line.\n",
    "class SentenceIterator:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def __iter__(self):\n",
    "        try:\n",
    "            with open(self.filepath, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    # Split the line into tokens based on whitespace\n",
    "                    tokens = line.split()\n",
    "                    if tokens: # Avoid yielding empty lists if there are blank lines\n",
    "                        yield tokens\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Error: Input corpus file not found at {self.filepath}\")\n",
    "            raise # Re-raise the exception to stop execution\n",
    "\n",
    "\n",
    "# --- Train the Model ---\n",
    "print(f\"Starting Word2Vec training...\")\n",
    "print(f\"Parameters: vector_size={vector_size}, window={window}, min_count={min_count}, sg={sg}, negative={negative}, workers={workers}, epochs={epochs}\")\n",
    "print(f\"Reading corpus from: {input_corpus_file}\")\n",
    "\n",
    "# Instantiate the sentence iterator\n",
    "sentences = SentenceIterator(input_corpus_file)\n",
    "\n",
    "# Instantiate and train the Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    sg=sg,\n",
    "    negative=negative,\n",
    "    workers=workers,\n",
    "    epochs=epochs # Use 'iter' in older gensim versions\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Save the Model ---\n",
    "print(f\"Saving model to: {output_model_file}\")\n",
    "model.save(output_model_file)\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# --- Basic Model Testing (Optional) ---\n",
    "# You can now load the model later using: model = Word2Vec.load(output_model_file)\n",
    "\n",
    "# Check vocabulary size\n",
    "vocab_size = len(model.wv.key_to_index) # Use model.wv.index_to_key in newer gensim\n",
    "print(f\"\\nVocabulary size: {vocab_size} unique tokens (after applying min_count={min_count})\")\n",
    "\n",
    "# Example: Find words similar to a given word (if it's in the vocabulary)\n",
    "# Try some common words from your romanized corpus analysis\n",
    "example_words = ['ana', 'bzaf', 'mzn', 'maroc'] # Adjust based on your actual frequent tokens\n",
    "print(\"\\nTesting similarity for some example words:\")\n",
    "\n",
    "for word in example_words:\n",
    "    if word in model.wv.key_to_index: # Use model.wv.__contains__ in newer gensim\n",
    "        try:\n",
    "            similar_words = model.wv.most_similar(word, topn=5)\n",
    "            print(f\"Words most similar to '{word}': {similar_words}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not get similar words for '{word}': {e}\")\n",
    "    else:\n",
    "        print(f\"Word '{word}' not found in the vocabulary (likely below min_count or not in corpus).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
