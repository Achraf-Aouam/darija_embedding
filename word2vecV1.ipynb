{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "# Optional: If you want to use all available cores for faster training\n",
    "import multiprocessing\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file: Contains the preprocessed text (one sentence/document per line, tokens separated by space)\n",
    "# This file was generated by your previous preprocessing script.\n",
    "input_corpus_file = '/teamspace/studios/this_studio/preprocessed_darija_corpus_arabic.txt'\n",
    "\n",
    "# Output file: Where to save the trained Word2Vec model\n",
    "output_model_file = 'darija_word2vec_sg_ns.model'\n",
    "\n",
    "# Word2Vec Parameters (as requested)\n",
    "vector_size = 100  # Dimensionality of the word vectors (common: 100-300)\n",
    "window = 5         # Max distance between current and predicted word within a sentence\n",
    "min_count = 8      # Ignores all words with total frequency lower than this\n",
    "sg = 1             # Use Skip-Gram algorithm (0 for CBOW)\n",
    "negative = 5       # Number of negative samples (common: 5-20)\n",
    "workers = multiprocessing.cpu_count() # Number of worker threads to train the model (use all available cores)\n",
    "epochs = 5        # Number of iterations (epochs) over the corpus (common: 5-10)\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Gensim uses Python's logging module to report progress\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# --- Create Sentence Iterator ---\n",
    "# Word2Vec requires the input corpus to be an iterable of lists of tokens (sentences).\n",
    "# We create a simple iterator class that reads your preprocessed file line-by-line.\n",
    "class SentenceIterator:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def __iter__(self):\n",
    "        try:\n",
    "            with open(self.filepath, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    # Split the line into tokens based on whitespace\n",
    "                    tokens = line.split()\n",
    "                    if tokens: # Avoid yielding empty lists if there are blank lines\n",
    "                        yield tokens\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Error: Input corpus file not found at {self.filepath}\")\n",
    "            raise # Re-raise the exception to stop execution\n",
    "\n",
    "\n",
    "# --- Train the Model ---\n",
    "print(f\"Starting Word2Vec training...\")\n",
    "print(f\"Parameters: vector_size={vector_size}, window={window}, min_count={min_count}, sg={sg}, negative={negative}, workers={workers}, epochs={epochs}\")\n",
    "print(f\"Reading corpus from: {input_corpus_file}\")\n",
    "\n",
    "# Instantiate the sentence iterator\n",
    "sentences = SentenceIterator(input_corpus_file)\n",
    "\n",
    "# Instantiate and train the Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    sg=sg,\n",
    "    negative=negative,\n",
    "    workers=workers,\n",
    "    epochs=epochs # Use 'iter' in older gensim versions\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Save the Model ---\n",
    "print(f\"Saving model to: {output_model_file}\")\n",
    "model.save(output_model_file)\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# --- Basic Model Testing (Optional) ---\n",
    "# You can now load the model later using: model = Word2Vec.load(output_model_file)\n",
    "\n",
    "# Check vocabulary size\n",
    "vocab_size = len(model.wv.key_to_index) # Use model.wv.index_to_key in newer gensim\n",
    "print(f\"\\nVocabulary size: {vocab_size} unique tokens (after applying min_count={min_count})\")\n",
    "\n",
    "# Example: Find words similar to a given word (if it's in the vocabulary)\n",
    "# Try some common words from your romanized corpus analysis\n",
    "example_words = ['عيقتو', 'بزاف', 'سكواتش', 'استغلال'] # Adjust based on your actual frequent tokens\n",
    "print(\"\\nTesting similarity for some example words:\")\n",
    "\n",
    "for word in example_words:\n",
    "    if word in model.wv.key_to_index: # Use model.wv.__contains__ in newer gensim\n",
    "        try:\n",
    "            similar_words = model.wv.most_similar(word, topn=5)\n",
    "            print(f\"Words most similar to '{word}': {similar_words}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not get similar words for '{word}': {e}\")\n",
    "    else:\n",
    "        print(f\"Word '{word}' not found in the vocabulary (likely below min_count or not in corpus).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
