{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import glob\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import json # For tokenizer files and potentially configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional dependencies\n",
    "try:\n",
    "    from langdetect import detect, LangDetectException, DetectorFactory\n",
    "    DetectorFactory.seed = 0  # for reproducible results\n",
    "    LANGDETECT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LANGDETECT_AVAILABLE = False\n",
    "    print(\"Warning: langdetect library not found. French detection step will be unavailable.\")\n",
    "\n",
    "try:\n",
    "    import arabic_reshaper\n",
    "    from bidi.algorithm import get_display\n",
    "    RTL_DISPLAY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RTL_DISPLAY_AVAILABLE = False\n",
    "    print(\"Warning: arabic_reshaper or python-bidi not found. RTL text in plots may not render correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_files(directory_path: str) -> str:\n",
    "    \"\"\"Reads all .txt files from a directory and concatenates their content.\"\"\"\n",
    "    all_text = [] # Use a list for efficiency, then join\n",
    "    file_paths = glob.glob(os.path.join(directory_path, \"*.txt\"))\n",
    "\n",
    "    if not file_paths:\n",
    "        # Return empty string instead of raising ValueError, pipeline can handle it\n",
    "        print(f\"Warning: No .txt files found in {directory_path}\")\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"Found {len(file_paths)} text files in '{directory_path}'.\")\n",
    "    for file_path in tqdm(file_paths, desc=\"Reading files\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                all_text.append(f.read())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    return \"\\n\".join(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_initial(text: str) -> str:\n",
    "    \"\"\"Performs initial cleaning of the text.\"\"\"\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    # Remove file markup like [[File:…]]\n",
    "    text = re.sub(r'\\[\\[File:[^\\]]*\\]\\]', ' ', text)\n",
    "    # Remove other generic wiki-like markups (e.g., [[...]], but not [[word]])\n",
    "    # This regex looks for markups with colons or pipes, common in metadata\n",
    "    text = re.sub(r'\\[\\[(?:[^\\]]*:|[^\\]]*\\|[^\\]]*)\\]\\]', ' ', text)\n",
    "    # Remove simple [[markup]] if it's not just a word\n",
    "    text = re.sub(r'\\[\\[([^\\]]{20,})\\]\\]', ' ', text) # Example: if content is too long\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove script-mismatches (e.g., \"100px\")\n",
    "    text = re.sub(r'\\b\\d+px\\b', ' ', text)\n",
    "    # Normalize Unicode to NFKC form for consistency\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    # Normalize whitespace early\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detect french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_filter_french_lines(\n",
    "    lines: list[str],\n",
    "    french_removal_threshold: float = 0.85, # If >85% of words are French-like\n",
    "    min_words_for_check: int = 4\n",
    "    ) -> list[str]:\n",
    "    \"\"\"\n",
    "    Detects and filters out lines that are predominantly French.\n",
    "    Keeps lines with mixed content or primarily Arabizi/Darija.\n",
    "    \"\"\"\n",
    "    if not LANGDETECT_AVAILABLE:\n",
    "        print(\"Skipping French detection: langdetect library not available.\")\n",
    "        return lines\n",
    "\n",
    "    kept_lines = []\n",
    "    removed_count = 0\n",
    "    print(f\"Analyzing {len(lines)} lines for French content...\")\n",
    "\n",
    "    for line in tqdm(lines, desc=\"Filtering French lines\"):\n",
    "        if not line.strip():\n",
    "            kept_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        words = line.split()\n",
    "        if len(words) < min_words_for_check:\n",
    "            kept_lines.append(line) # Keep short lines, likely not full French sentences\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Use langdetect on the line\n",
    "            lang = detect(line)\n",
    "            if lang == 'fr':\n",
    "                # Further heuristic: check proportion of purely Latin words\n",
    "                # (without Arabizi numbers like 2,3,5,7,9)\n",
    "                latin_word_count = 0\n",
    "                arabizi_signal_present = False\n",
    "                for word in words:\n",
    "                    if re.search(r'[a-zA-Z]', word) and not re.search(r'[0-9]', word): # Simple check for Latin word\n",
    "                        latin_word_count += 1\n",
    "                    if re.search(r'[23579]', word): # Arabizi digits\n",
    "                        arabizi_signal_present = True\n",
    "                \n",
    "                # If it's detected as French AND has many Latin words AND no strong Arabizi digit signal,\n",
    "                # it's more likely to be \"pure\" French.\n",
    "                # This is a heuristic. A simpler rule is just if lang == 'fr'.\n",
    "                # For now, let's use a simpler rule: if langdetect says 'fr', it's likely French.\n",
    "                # Darija with many French words might be misclassified, but the goal is to remove \"pure French\".\n",
    "                \n",
    "                # If the line is detected as French and does not seem to contain Arabizi markers (like 3, 7 etc.)\n",
    "                # it's a stronger candidate for removal.\n",
    "                # A simple approach: if langdetect says 'fr', remove.\n",
    "                # If it contains Arabizi markers like '3', '7', it's likely Darija.\n",
    "                if not any(c in line for c in \"23579\"): # if no common Arabizi numbers\n",
    "                    removed_count += 1\n",
    "                else: # It's 'fr' but has Arabizi numbers, likely Darija\n",
    "                    kept_lines.append(line)\n",
    "            else:\n",
    "                # If not 'fr' (e.g., 'ar', 'en' for Arabizi, etc.), keep it.\n",
    "                kept_lines.append(line)\n",
    "        except LangDetectException:\n",
    "            # If language cannot be detected (e.g., too short, mixed symbols), keep it by default\n",
    "            kept_lines.append(line)\n",
    "            \n",
    "    print(f\"Removed {removed_count} lines identified as predominantly French.\")\n",
    "    return kept_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARABIZI_TO_ARABIC_MAP = {\n",
    "    # Digits (ensure these are processed before any general digit removal)\n",
    "    '2': 'ء', '3': 'ع', '4': 'غ', '6': 'ط', '8': 'ق', # Added some other common ones\n",
    "    '7': 'ح', '5': 'خ', '9': 'ق', # 9 can be ق or ص, user asked for ق, but ص is also common. Let's use ق as requested.\n",
    "                                # The user's original request had '9' -> 'ق'. I'll stick to that. '5' -> 'خ'\n",
    "    # Common multi-character sequences (longest first for correct replacement)\n",
    "    'ch': 'ش', 'sh': 'ش', 'kh': 'خ', 'gh': 'غ',\n",
    "    'th': 'ث', 'dh': 'ذ', 'ou': 'و', 'oo': 'و',\n",
    "    # Single letters (ensure input text is lowercased before this)\n",
    "    'a': 'ا', 'b': 'ب', 'c': 'س', # 'c' can be tricky, 'س' is a common default\n",
    "    'd': 'د', 'e': 'ي', # 'e' often like 'i' or kasra, 'ي' is a placeholder. Can also be 'ا'.\n",
    "    'f': 'ف', 'g': 'ڭ', # Moroccan Gaf. normalize_arabic can convert ڭ to ك or ج later if needed.\n",
    "    'h': 'ه', 'i': 'ي', 'j': 'ج', 'k': 'ك', 'l': 'ل', 'm': 'م',\n",
    "    'n': 'ن', 'o': 'و', 'p': 'ب', # 'پ' is not standard, so 'ب'\n",
    "    'q': 'ق', 'r': 'ر', 's': 'س',\n",
    "    't': 'ت', 'u': 'و', 'v': 'ف', # 'ڤ' is not standard, so 'ف'\n",
    "    'w': 'و', 'x': 'كس', 'y': 'ي', 'z': 'ز',\n",
    "}\n",
    "# # Add user's specific request for 9->ق\n",
    "# ARABIZI_TO_ARABIC_MAP['9'] = 'ق'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arabize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabize_text(text: str) -> str:\n",
    "    \"\"\"Converts Arabizi (Latin script Darija with numbers) to Arabic script.\"\"\"\n",
    "    text = text.lower() # Important for consistent mapping\n",
    "\n",
    "    # Create a list of keys sorted by length (descending) to handle multi-char keys first\n",
    "    sorted_keys = sorted(ARABIZI_TO_ARABIC_MAP.keys(), key=len, reverse=True)\n",
    "\n",
    "    for key in sorted_keys:\n",
    "        text = text.replace(key, ARABIZI_TO_ARABIC_MAP[key])\n",
    "    \n",
    "    # Specific case for 'g', if it was mapped to 'ڭ' and needs to be 'غ' or 'ج'\n",
    "    # For now, 'ڭ' is kept, and normalize_arabic can handle it.\n",
    "    # If user specifically wants 'g' -> 'غ', then ARABIZI_TO_ARABIC_MAP['g'] = 'غ'\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic_text(text: str) -> str:\n",
    "    \"\"\"Normalizes Arabic script.\"\"\"\n",
    "    # Remove diacritics (tashkeel)\n",
    "    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n",
    "    # Remove tatweel (ـ)\n",
    "    text = text.replace('\\u0640', '')\n",
    "    \n",
    "    # Normalize Alef forms to plain Alef (ا)\n",
    "    text = text.replace('أ', 'ا').replace('إ', 'ا').replace('آ', 'ا').replace('ٱ', 'ا')\n",
    "    \n",
    "    # Normalize common variants\n",
    "    text = text.replace('ة', 'ه')  # Ta marbuta to Ha\n",
    "    text = text.replace('ى', 'ي')  # Alef maksura to Ya\n",
    "    \n",
    "    # Normalize Perso-Arabic letters to common Arabic equivalents if desired\n",
    "    text = text.replace('گ', 'ك')  # Persian Gaf to Kaf\n",
    "    text = text.replace('ڭ', 'ك')  # Moroccan Gaf (if produced by arabize_text) to Kaf. Or map to ج or غ if preferred.\n",
    "                                  # Let's map ڭ to ك as it's a common normalization.\n",
    "    text = text.replace('چ', 'ش')  # Cheh to Shin (if 'ch' was mapped to 'چ')\n",
    "    text = text.replace('پ', 'ب')  # Peh to Ba\n",
    "    text = text.replace('ڤ', 'ف')  # Veh to Fa\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_final(text: str, remove_all_numbers: bool = True) -> str:\n",
    "    \"\"\"Performs final cleaning, focusing on script and remaining numbers.\"\"\"\n",
    "    if remove_all_numbers:\n",
    "        # Remove standalone numbers (Western or Arabic-Indic if any were produced)\n",
    "        text = re.sub(r'\\b\\d+\\b', ' ', text) # Western digits\n",
    "        text = re.sub(r'\\b[\\u0660-\\u0669]+\\b', ' ', text) # Arabic-Indic digits\n",
    "        text = re.sub(r'\\(\\d+\\)', ' ', text)\n",
    "        text = re.sub(r'\\([\\u0660-\\u0669]+\\)', ' ', text)\n",
    "\n",
    "    # Keep only Arabic letters and whitespace.\n",
    "    # This assumes all desired text is now in Arabic script.\n",
    "    text = re.sub(r'[^\\p{Arabic}\\s]', ' ', text, flags=re.UNICODE)\n",
    "    \n",
    "    # Normalize whitespace again\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_tokens_from_lines(\n",
    "    lines: list[str],\n",
    "    min_token_length: int,\n",
    "    short_token_allowlist: set = None\n",
    "    ) -> tuple[list[str], int, int]:\n",
    "    \"\"\"\n",
    "    Filters out tokens shorter than min_token_length from each line,\n",
    "    unless they are in the allowlist.\n",
    "    Returns filtered lines and token counts.\n",
    "    \"\"\"\n",
    "    if short_token_allowlist is None:\n",
    "        short_token_allowlist = set()\n",
    "\n",
    "    filtered_lines = []\n",
    "    original_token_count = 0\n",
    "    final_token_count = 0\n",
    "\n",
    "    print(f\"Filtering tokens shorter than {min_token_length} characters (allowlist: {short_token_allowlist})...\")\n",
    "    for line in tqdm(lines, desc=\"Filtering short tokens\"):\n",
    "        tokens = line.split()\n",
    "        original_token_count += len(tokens)\n",
    "        \n",
    "        kept_tokens = [\n",
    "            token for token in tokens\n",
    "            if len(token) >= min_token_length or token in short_token_allowlist\n",
    "        ]\n",
    "        \n",
    "        final_token_count += len(kept_tokens)\n",
    "        if kept_tokens: # Only add line if it's not empty after filtering\n",
    "            filtered_lines.append(\" \".join(kept_tokens))\n",
    "    \n",
    "    print(f\"Original token count (whitespace split): {original_token_count}\")\n",
    "    print(f\"Token count after filtering short tokens: {final_token_count}\")\n",
    "    filtered_out_count = original_token_count - final_token_count\n",
    "    print(f\"Filtered out {filtered_out_count} short tokens.\")\n",
    "    \n",
    "    if not filtered_lines and original_token_count > 0 :\n",
    "         print(\"WARNING: All lines became empty after filtering short tokens. Check min_token_length or corpus.\")\n",
    "    return filtered_lines, original_token_count, final_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_tokenizer(\n",
    "    lines_for_bpe: list[str],\n",
    "    vocab_size: int,\n",
    "    min_frequency: int,\n",
    "    bpe_tokenizer_output_file: str,\n",
    "    temp_bpe_train_file: str\n",
    "    ) -> Tokenizer:\n",
    "    \"\"\"Trains a BPE tokenizer on the provided lines of text.\"\"\"\n",
    "    print(f\"Training BPE tokenizer (Vocab Size: {vocab_size}, Min Freq: {min_frequency})...\")\n",
    "    \n",
    "    with open(temp_bpe_train_file, 'w', encoding='utf-8') as f:\n",
    "        for line in tqdm(lines_for_bpe, desc=\"Writing lines for BPE training\"):\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace() # Respect existing word boundaries initially\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    )\n",
    "    \n",
    "    tokenizer.train([temp_bpe_train_file], trainer)\n",
    "    tokenizer.save(bpe_tokenizer_output_file)\n",
    "    print(f\"BPE Tokenizer trained and saved to {bpe_tokenizer_output_file}\")\n",
    "    \n",
    "    if os.path.exists(temp_bpe_train_file):\n",
    "        os.remove(temp_bpe_train_file)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bpe_and_save_corpus(\n",
    "    lines_to_tokenize: list[str],\n",
    "    tokenizer_path: str, # Path to the saved tokenizer.json\n",
    "    final_tokenized_output_file: str\n",
    "    ) -> int:\n",
    "    \"\"\"Applies a trained BPE tokenizer to lines and saves the tokenized corpus.\"\"\"\n",
    "    print(f\"Applying BPE tokenizer from {tokenizer_path}...\")\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "    total_bpe_tokens = 0\n",
    "\n",
    "    with open(final_tokenized_output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for line in tqdm(lines_to_tokenize, desc=\"Tokenizing with BPE\"):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            encoding = tokenizer.encode(line)\n",
    "            bpe_tokens = encoding.tokens\n",
    "            f_out.write(\" \".join(bpe_tokens) + '\\n')\n",
    "            total_bpe_tokens += len(bpe_tokens)\n",
    "            \n",
    "    print(f\"Finished BPE tokenizing. Saved {total_bpe_tokens} BPE tokens to {final_tokenized_output_file}\")\n",
    "    return total_bpe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_token_corpus(\n",
    "    token_source: str | list[str], # Can be file path or list of tokens\n",
    "    top_n: int = 30,\n",
    "    is_bpe_corpus: bool = False\n",
    "    ) -> pd.DataFrame | None:\n",
    "    \"\"\"Analyzes token frequencies and lengths from a file or list of tokens.\"\"\"\n",
    "    all_tokens = []\n",
    "    if isinstance(token_source, str): # It's a file path\n",
    "        print(f\"Reading tokens from {token_source} for analysis...\")\n",
    "        try:\n",
    "            with open(token_source, 'r', encoding='utf-8') as f_in:\n",
    "                for line in tqdm(f_in, desc=\"Reading tokens for analysis\"):\n",
    "                    all_tokens.extend(line.strip().split())\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find {token_source} for analysis.\")\n",
    "            return None\n",
    "    elif isinstance(token_source, list):\n",
    "        all_tokens = token_source\n",
    "    else:\n",
    "        print(\"Error: token_source must be a file path or a list of tokens.\")\n",
    "        return None\n",
    "\n",
    "    if not all_tokens:\n",
    "        print(\"Warning: No tokens provided or found for analysis.\")\n",
    "        return None\n",
    "\n",
    "    corpus_type = \"BPE Token\" if is_bpe_corpus else \"Word Token\"\n",
    "    print(f\"Analyzing {len(all_tokens)} {corpus_type.lower()}s...\")\n",
    "\n",
    "    token_freq = {}\n",
    "    for token in tqdm(all_tokens, desc=\"Calculating Frequencies\"):\n",
    "        token_freq[token] = token_freq.get(token, 0) + 1\n",
    "\n",
    "    df = pd.DataFrame.from_dict(token_freq, orient='index', columns=['frequency'])\n",
    "    df = df.sort_values('frequency', ascending=False).reset_index()\n",
    "    df.columns = ['token', 'frequency']\n",
    "    df['length'] = df['token'].apply(len)\n",
    "    total_token_occurrences = df['frequency'].sum() # Sum of frequencies is total tokens\n",
    "    df['rel_frequency'] = df['frequency'] / total_token_occurrences\n",
    "\n",
    "    print(\"Generating plots...\")\n",
    "    plt.style.use('seaborn-v0_8-whitegrid') # Example style\n",
    "\n",
    "    # Plot top token frequencies\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plot_data = df.head(top_n).copy()\n",
    "    plot_data['token_str'] = plot_data['token'].astype(str) # Ensure string type\n",
    "\n",
    "    # Handle RTL display for Arabic tokens\n",
    "    if RTL_DISPLAY_AVAILABLE and plot_data['token_str'].apply(lambda x: bool(re.search(r'[\\u0600-\\u06FF]', x))).any():\n",
    "        plot_data['token_display'] = plot_data['token_str'].apply(\n",
    "            lambda x: get_display(arabic_reshaper.reshape(x))\n",
    "        )\n",
    "        sns.barplot(x='token_display', y='frequency', data=plot_data, palette=\"viridis\")\n",
    "    else:\n",
    "        sns.barplot(x='token_str', y='frequency', data=plot_data, palette=\"viridis\")\n",
    "    \n",
    "    plt.title(f'Top {top_n} {corpus_type} Frequencies', fontsize=16)\n",
    "    plt.xlabel(corpus_type, fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.xticks(rotation=60, ha='right', fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot token length distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['length'], bins=max(1, df['length'].max()), kde=False, color=\"skyblue\")\n",
    "    plt.title(f'{corpus_type} Length Distribution', fontsize=16)\n",
    "    plt.xlabel(f'{corpus_type} Length', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    # common_max_len = df['length'].quantile(0.99)\n",
    "    # plt.xlim(0, min(common_max_len + 1, df['length'].max() + 1)) # Adjust x-limit\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nTop {top_n} {corpus_type}s (from analysis):\")\n",
    "    print(df[['token', 'frequency']].head(top_n).to_string())\n",
    "    print(\"Analysis complete.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darija_preprocessing_pipeline(\n",
    "    corpus_dir: str,\n",
    "    output_dir: str = \"processed_darija_corpus\",\n",
    "    sample_size_chars: int | None = None, # Max characters to process from raw text\n",
    "    \n",
    "    # Step controls\n",
    "    do_initial_clean: bool = True,\n",
    "    do_filter_french: bool = True,\n",
    "    do_arabize: bool = True,\n",
    "    do_normalize_arabic: bool = True,\n",
    "    do_final_clean: bool = True,\n",
    "    do_filter_short_tokens: bool = True,\n",
    "    do_bpe_tokenize: bool = True,\n",
    "    do_analyze_final_corpus: bool = True,\n",
    "\n",
    "    # Configuration for steps\n",
    "    french_filter_config: dict | None = None,\n",
    "    short_token_filter_config: dict | None = None,\n",
    "    bpe_config: dict | None = None,\n",
    "    analysis_config: dict | None = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Main pipeline for preprocessing Darija corpus.\n",
    "    Allows enabling/disabling and configuring each step.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Pipeline started. Output will be in '{output_dir}'\")\n",
    "\n",
    "    # --- Default configurations ---\n",
    "    _french_filter_config = {'french_removal_threshold': 0.85, 'min_words_for_check': 4}\n",
    "    if french_filter_config: _french_filter_config.update(french_filter_config)\n",
    "\n",
    "    _short_token_filter_config = {'min_token_length': 2, 'allowlist': {'و', 'في', 'من', 'الى', 'لا', 'ما', 'عن', 'مع'}} # Arabic allowlist\n",
    "    if short_token_filter_config: _short_token_filter_config.update(short_token_filter_config)\n",
    "    \n",
    "    _bpe_config = {'vocab_size': 30000, 'min_frequency': 2}\n",
    "    if bpe_config: _bpe_config.update(bpe_config)\n",
    "\n",
    "    _analysis_config = {'top_n': 30}\n",
    "    if analysis_config: _analysis_config.update(analysis_config)\n",
    "\n",
    "    # --- File paths ---\n",
    "    temp_bpe_train_file = os.path.join(output_dir, \"temp_corpus_for_bpe_training.txt\")\n",
    "    bpe_tokenizer_file = os.path.join(output_dir, \"darija_bpe_tokenizer.json\")\n",
    "    \n",
    "    if do_bpe_tokenize:\n",
    "        final_corpus_path = os.path.join(output_dir, \"darija_bpe_tokenized_for_w2v.txt\")\n",
    "    else:\n",
    "        final_corpus_path = os.path.join(output_dir, \"darija_processed_words_for_w2v.txt\")\n",
    "\n",
    "    # === PIPELINE STEPS ===\n",
    "\n",
    "    # 1. Read Corpus\n",
    "    print(\"\\n--- Step 1: Reading Corpus ---\")\n",
    "    raw_text = read_corpus_files(corpus_dir)\n",
    "    if not raw_text:\n",
    "        print(\"No text read from corpus. Exiting pipeline.\")\n",
    "        return None, None\n",
    "    \n",
    "    if sample_size_chars and len(raw_text) > sample_size_chars:\n",
    "        print(f\"Sampling {sample_size_chars} characters from the corpus.\")\n",
    "        raw_text = raw_text[:sample_size_chars]\n",
    "    print(f\"Initial corpus size: {len(raw_text)} characters.\")\n",
    "\n",
    "    processed_text = raw_text\n",
    "\n",
    "    # 2. Initial Cleaning\n",
    "    if do_initial_clean:\n",
    "        print(\"\\n--- Step 2: Initial Text Cleaning ---\")\n",
    "        processed_text = clean_text_initial(processed_text)\n",
    "        print(f\"Text size after initial cleaning: {len(processed_text)} characters.\")\n",
    "\n",
    "    # Convert text to lines for line-based processing\n",
    "    current_lines = processed_text.splitlines()\n",
    "    current_lines = [line for line in current_lines if line.strip()] # Remove empty lines\n",
    "\n",
    "    # 3. Filter French Lines\n",
    "    if do_filter_french:\n",
    "        print(\"\\n--- Step 3: Filtering French Lines ---\")\n",
    "        if not LANGDETECT_AVAILABLE:\n",
    "            print(\"Skipping French filtering as langdetect is not available.\")\n",
    "        else:\n",
    "            current_lines = detect_and_filter_french_lines(\n",
    "                current_lines,\n",
    "                **_french_filter_config\n",
    "            )\n",
    "            print(f\"{len(current_lines)} lines remaining after French filtering.\")\n",
    "\n",
    "    # Join lines back to text for text-based processing\n",
    "    processed_text = \"\\n\".join(current_lines)\n",
    "\n",
    "    # 4. Arabize Text (Latin/Arabizi to Arabic script)\n",
    "    if do_arabize:\n",
    "        print(\"\\n--- Step 4: Arabizing Text ---\")\n",
    "        processed_text = arabize_text(processed_text)\n",
    "        print(f\"Text size after arabization: {len(processed_text)} characters.\")\n",
    "        # print(\"Sample after arabization:\", processed_text[:500])\n",
    "\n",
    "\n",
    "    # 5. Normalize Arabic Script\n",
    "    if do_normalize_arabic:\n",
    "        print(\"\\n--- Step 5: Normalizing Arabic Script ---\")\n",
    "        processed_text = normalize_arabic_text(processed_text)\n",
    "        print(f\"Text size after Arabic normalization: {len(processed_text)} characters.\")\n",
    "        # print(\"Sample after Arabic normalization:\", processed_text[:500])\n",
    "\n",
    "    # 6. Final Cleaning (remove remaining non-Arabic, numbers if configured)\n",
    "    if do_final_clean:\n",
    "        print(\"\\n--- Step 6: Final Text Cleaning ---\")\n",
    "        processed_text = clean_text_final(processed_text, remove_all_numbers=True)\n",
    "        print(f\"Text size after final cleaning: {len(processed_text)} characters.\")\n",
    "        # print(\"Sample after final cleaning:\", processed_text[:500])\n",
    "\n",
    "    # Convert text back to lines for token-based filtering and BPE\n",
    "    current_lines = processed_text.splitlines()\n",
    "    current_lines = [line for line in current_lines if line.strip()]\n",
    "\n",
    "    # 7. Filter Short Tokens\n",
    "    if do_filter_short_tokens:\n",
    "        print(\"\\n--- Step 7: Filtering Short Tokens ---\")\n",
    "        current_lines, _, _ = filter_short_tokens_from_lines(\n",
    "            current_lines,\n",
    "            min_token_length=_short_token_filter_config['min_token_length'],\n",
    "            short_token_allowlist=_short_token_filter_config['allowlist']\n",
    "        )\n",
    "        print(f\"{len(current_lines)} lines remaining after short token filtering.\")\n",
    "        if not current_lines and processed_text: # Check if all content was filtered\n",
    "             print(\"Warning: No lines remaining after short token filtering. Check configuration or corpus state.\")\n",
    "             # Decide if to stop or continue with empty lines for BPE (which will likely fail or be empty)\n",
    "             # For now, we'll let it proceed, BPE training will handle empty input.\n",
    "\n",
    "    # 8. BPE Tokenization (Train and Apply) OR Save Word-Tokenized Corpus\n",
    "    if do_bpe_tokenize:\n",
    "        print(\"\\n--- Step 8: BPE Tokenization ---\")\n",
    "        if not current_lines:\n",
    "            print(\"No lines available for BPE training. Skipping BPE.\")\n",
    "            final_corpus_path = None # Indicate no BPE corpus generated\n",
    "            # Optionally, save an empty file or the word-tokenized (empty) version\n",
    "            with open(os.path.join(output_dir, \"darija_processed_words_empty.txt\"), 'w') as f:\n",
    "                f.write(\"\") # Create an empty file\n",
    "        else:\n",
    "            trained_tokenizer = train_bpe_tokenizer(\n",
    "                current_lines,\n",
    "                vocab_size=_bpe_config['vocab_size'],\n",
    "                min_frequency=_bpe_config['min_frequency'],\n",
    "                bpe_tokenizer_output_file=bpe_tokenizer_file,\n",
    "                temp_bpe_train_file=temp_bpe_train_file\n",
    "            )\n",
    "            _ = apply_bpe_and_save_corpus(\n",
    "                current_lines,\n",
    "                bpe_tokenizer_file,\n",
    "                final_corpus_path\n",
    "            )\n",
    "    else:\n",
    "        print(\"\\n--- Step 8: Saving Word-Tokenized Corpus (BPE Skipped) ---\")\n",
    "        if not current_lines:\n",
    "            print(\"No lines available to save for word-tokenized corpus.\")\n",
    "            with open(final_corpus_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"\") # Save an empty file\n",
    "            print(f\"Saved empty word-tokenized corpus to {final_corpus_path}\")\n",
    "        else:\n",
    "            word_count = 0\n",
    "            with open(final_corpus_path, 'w', encoding='utf-8') as f:\n",
    "                for line in tqdm(current_lines, desc=\"Saving word-tokenized corpus\"):\n",
    "                    f.write(line + '\\n')\n",
    "                    word_count += len(line.split())\n",
    "            print(f\"Saved {word_count} words in {len(current_lines)} lines to {final_corpus_path}\")\n",
    "\n",
    "    # 9. Analyze Final Corpus\n",
    "    if do_analyze_final_corpus and final_corpus_path and os.path.exists(final_corpus_path) and os.path.getsize(final_corpus_path) > 0:\n",
    "        print(\"\\n--- Step 9: Analyzing Final Corpus ---\")\n",
    "        analyze_token_corpus(\n",
    "            final_corpus_path,\n",
    "            top_n=_analysis_config['top_n'],\n",
    "            is_bpe_corpus=do_bpe_tokenize # True if BPE was done\n",
    "        )\n",
    "    elif do_analyze_final_corpus:\n",
    "        print(\"\\n--- Step 9: Analyzing Final Corpus ---\")\n",
    "        print(\"Skipping analysis: final corpus file is empty or does not exist.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Pipeline Finished ---\")\n",
    "    return final_corpus_path, bpe_tokenizer_file if do_bpe_tokenize else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (in a separate script or IPython/Jupyter notebook)\n",
    "# from darija_processor import darija_preprocessing_pipeline # If saved as darija_processor.py\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CORPUS_DIRECTORY = 'data' # Change to your corpus directory path\n",
    "    OUTPUT_DIRECTORY = 'processed_darija_v2'\n",
    "    \n",
    "    # Example: Run with most steps enabled\n",
    "    final_file, tokenizer_model = darija_preprocessing_pipeline(\n",
    "        corpus_dir=CORPUS_DIRECTORY,\n",
    "        output_dir=OUTPUT_DIRECTORY,\n",
    "        sample_size_chars=None, # Process first 10 million characters, or None for full\n",
    "        \n",
    "        do_initial_clean=True,\n",
    "        do_filter_french=True, # Requires langdetect\n",
    "        do_arabize=True,\n",
    "        do_normalize_arabic=True,\n",
    "        do_final_clean=True,\n",
    "        do_filter_short_tokens=True,\n",
    "        do_bpe_tokenize=True, # Set to False to get word tokens instead of BPE\n",
    "        do_analyze_final_corpus=True,\n",
    "\n",
    "        # --- You can customize configurations here ---\n",
    "        # french_filter_config={'min_words_for_check': 3},\n",
    "        short_token_filter_config={\n",
    "            'min_token_length': 3, # Keep words of length 2 or more\n",
    "            'allowlist': {},\n",
    "            # 'allowlist': {'و', 'في', 'من', 'لا', 'ما', 'يا', 'له', 'به', 'هو'}, # Common short Arabic words\n",
    "        },\n",
    "        bpe_config={'vocab_size': 50000, 'min_frequency': 3},\n",
    "        analysis_config={'top_n': 25}\n",
    "    )\n",
    "\n",
    "    if final_file:\n",
    "        print(f\"\\nPreprocessing complete.\")\n",
    "        print(f\"Final tokenized corpus for Word2Vec: {final_file}\")\n",
    "        if tokenizer_model:\n",
    "            print(f\"BPE tokenizer model saved to: {tokenizer_model}\")\n",
    "    else:\n",
    "        print(\"\\nPreprocessing did not produce a final corpus file.\")\n",
    "\n",
    "    # Example: Run without BPE, keeping only words of length 3+\n",
    "    # final_file_no_bpe, _ = darija_preprocessing_pipeline(\n",
    "    #     corpus_dir=CORPUS_DIRECTORY,\n",
    "    #     output_dir=os.path.join(OUTPUT_DIRECTORY, \"no_bpe_run\"),\n",
    "    #     sample_size_chars=1000000,\n",
    "    #     do_bpe_tokenize=False,\n",
    "    #     short_token_filter_config={'min_token_length': 3, 'allowlist': set()}\n",
    "    # )\n",
    "    # if final_file_no_bpe:\n",
    "    #     print(f\"\\nProcessed (no BPE) corpus: {final_file_no_bpe}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
