{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re # Make sure regex is imported if not already\n",
    "from gensim.models import Word2Vec\n",
    "from tokenizers import Tokenizer\n",
    "import unicodedata\n",
    "# Optional: For RTL display in terminal/output\n",
    "try:\n",
    "    import arabic_reshaper\n",
    "    from bidi.algorithm import get_display\n",
    "    RTL_DISPLAY_AVAILABLE = True\n",
    "    print(\"arabic_reshaper and python-bidi found. RTL display enabled.\")\n",
    "except ImportError:\n",
    "    RTL_DISPLAY_AVAILABLE = False\n",
    "    print(\"Warning: arabic_reshaper or python-bidi not found. RTL display might not be optimal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# *** Path to your trained Word2Vec model ***\n",
    "WORD2VEC_MODEL_PATH = 'darija_word2vec_bpe_sg_ns.model' # !!! UPDATE THIS PATH !!!\n",
    "# *** Path to your BPE tokenizer file ***\n",
    "BPE_TOKENIZER_PATH = 'processed_darija_v2/darija_bpe_tokenizer.json'      # !!! UPDATE THIS PATH !!!\n",
    "\n",
    "# ==============================================================================\n",
    "# PASTE YOUR PREPROCESSING FUNCTION DEFINITIONS HERE\n",
    "# Make sure the following functions (and ARABIZI_TO_ARABIC_MAP) are defined\n",
    "# in cells ABOVE this script in your notebook:\n",
    "#\n",
    "# 1. ARABIZI_TO_ARABIC_MAP = { ... }\n",
    "# 2. def clean_text_initial(text: str) -> str: ...\n",
    "# 3. def arabize_text(text: str) -> str: ...\n",
    "# 4. def normalize_arabic_text(text: str) -> str: ...\n",
    "#\n",
    "# (The script uses a simplified final clean, so clean_text_final is not strictly required here\n",
    "# unless your other functions depend on it in a way not covered)\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARABIZI_TO_ARABIC_MAP = {\n",
    "    # Digits (ensure these are processed before any general digit removal)\n",
    "    '2': 'ء', '3': 'ع', '4': 'غ', '6': 'ط', '8': 'ق', # Added some other common ones\n",
    "    '7': 'ح', '5': 'خ', '9': 'ق', # 9 can be ق or ص, user asked for ق, but ص is also common. Let's use ق as requested.\n",
    "                                # The user's original request had '9' -> 'ق'. I'll stick to that. '5' -> 'خ'\n",
    "    # Common multi-character sequences (longest first for correct replacement)\n",
    "    'ch': 'ش', 'sh': 'ش', 'kh': 'خ', 'gh': 'غ',\n",
    "    'th': 'ث', 'dh': 'ذ', 'ou': 'و', 'oo': 'و',\n",
    "    # Single letters (ensure input text is lowercased before this)\n",
    "    'a': 'ا', 'b': 'ب', 'c': 'س', # 'c' can be tricky, 'س' is a common default\n",
    "    'd': 'د', 'e': 'ي', # 'e' often like 'i' or kasra, 'ي' is a placeholder. Can also be 'ا'.\n",
    "    'f': 'ف', 'g': 'ڭ', # Moroccan Gaf. normalize_arabic can convert ڭ to ك or ج later if needed.\n",
    "    'h': 'ه', 'i': 'ي', 'j': 'ج', 'k': 'ك', 'l': 'ل', 'm': 'م',\n",
    "    'n': 'ن', 'o': 'و', 'p': 'ب', # 'پ' is not standard, so 'ب'\n",
    "    'q': 'ق', 'r': 'ر', 's': 'س',\n",
    "    't': 'ت', 'u': 'و', 'v': 'ف', # 'ڤ' is not standard, so 'ف'\n",
    "    'w': 'و', 'x': 'كس', 'y': 'ي', 'z': 'ز',\n",
    "}\n",
    "# # Add user's specific request for 9->ق\n",
    "# ARABIZI_TO_ARABIC_MAP['9'] = 'ق'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_initial(text: str) -> str:\n",
    "    \"\"\"Performs initial cleaning of the text.\"\"\"\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    # Remove file markup like [[File:…]]\n",
    "    text = re.sub(r'\\[\\[File:[^\\]]*\\]\\]', ' ', text)\n",
    "    # Remove other generic wiki-like markups (e.g., [[...]], but not [[word]])\n",
    "    # This regex looks for markups with colons or pipes, common in metadata\n",
    "    text = re.sub(r'\\[\\[(?:[^\\]]*:|[^\\]]*\\|[^\\]]*)\\]\\]', ' ', text)\n",
    "    # Remove simple [[markup]] if it's not just a word\n",
    "    text = re.sub(r'\\[\\[([^\\]]{20,})\\]\\]', ' ', text) # Example: if content is too long\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove script-mismatches (e.g., \"100px\")\n",
    "    text = re.sub(r'\\b\\d+px\\b', ' ', text)\n",
    "    # Normalize Unicode to NFKC form for consistency\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    # Normalize whitespace early\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabize_text(text: str) -> str:\n",
    "    \"\"\"Converts Arabizi (Latin script Darija with numbers) to Arabic script.\"\"\"\n",
    "    text = text.lower() # Important for consistent mapping\n",
    "\n",
    "    # Create a list of keys sorted by length (descending) to handle multi-char keys first\n",
    "    sorted_keys = sorted(ARABIZI_TO_ARABIC_MAP.keys(), key=len, reverse=True)\n",
    "\n",
    "    for key in sorted_keys:\n",
    "        text = text.replace(key, ARABIZI_TO_ARABIC_MAP[key])\n",
    "    \n",
    "    # Specific case for 'g', if it was mapped to 'ڭ' and needs to be 'غ' or 'ج'\n",
    "    # For now, 'ڭ' is kept, and normalize_arabic can handle it.\n",
    "    # If user specifically wants 'g' -> 'غ', then ARABIZI_TO_ARABIC_MAP['g'] = 'غ'\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic_text(text: str) -> str:\n",
    "    \"\"\"Normalizes Arabic script.\"\"\"\n",
    "    # Remove diacritics (tashkeel)\n",
    "    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n",
    "    # Remove tatweel (ـ)\n",
    "    text = text.replace('\\u0640', '')\n",
    "    \n",
    "    # Normalize Alef forms to plain Alef (ا)\n",
    "    text = text.replace('أ', 'ا').replace('إ', 'ا').replace('آ', 'ا').replace('ٱ', 'ا')\n",
    "    \n",
    "    # Normalize common variants\n",
    "    text = text.replace('ة', 'ه')  # Ta marbuta to Ha\n",
    "    text = text.replace('ى', 'ي')  # Alef maksura to Ya\n",
    "    \n",
    "    # Normalize Perso-Arabic letters to common Arabic equivalents if desired\n",
    "    text = text.replace('گ', 'ك')  # Persian Gaf to Kaf\n",
    "    text = text.replace('ڭ', 'ك')  # Moroccan Gaf (if produced by arabize_text) to Kaf. Or map to ج or غ if preferred.\n",
    "                                  # Let's map ڭ to ك as it's a common normalization.\n",
    "    text = text.replace('چ', 'ش')  # Cheh to Shin (if 'ch' was mapped to 'چ')\n",
    "    text = text.replace('پ', 'ب')  # Peh to Ba\n",
    "    text = text.replace('ڤ', 'ف')  # Veh to Fa\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Load Models ---\n",
    "word2vec_model = None\n",
    "bpe_tokenizer = None\n",
    "models_loaded_successfully = False\n",
    "\n",
    "try:\n",
    "    if os.path.exists(WORD2VEC_MODEL_PATH):\n",
    "        print(f\"Loading Word2Vec model from: {WORD2VEC_MODEL_PATH}\")\n",
    "        word2vec_model = Word2Vec.load(WORD2VEC_MODEL_PATH)\n",
    "        print(\"Word2Vec model loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Error: Word2Vec model file not found at {WORD2VEC_MODEL_PATH}\")\n",
    "\n",
    "    if os.path.exists(BPE_TOKENIZER_PATH):\n",
    "        print(f\"Loading BPE tokenizer from: {BPE_TOKENIZER_PATH}\")\n",
    "        bpe_tokenizer = Tokenizer.from_file(BPE_TOKENIZER_PATH)\n",
    "        print(\"BPE tokenizer loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Error: BPE tokenizer file not found at {BPE_TOKENIZER_PATH}\")\n",
    "\n",
    "    if word2vec_model and bpe_tokenizer:\n",
    "        models_loaded_successfully = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model loading: {e}\")\n",
    "\n",
    "\n",
    "def preprocess_input_word_for_similarity(text: str, tokenizer: Tokenizer) -> list[str]:\n",
    "    \"\"\"\n",
    "    Applies necessary preprocessing to a single input word/phrase\n",
    "    and returns its BPE tokens.\n",
    "    Assumes clean_text_initial, arabize_text, normalize_arabic_text are defined.\n",
    "    \"\"\"\n",
    "    # 1. Initial clean (less critical for single words, but for consistency)\n",
    "    try:\n",
    "        processed_text = clean_text_initial(text)\n",
    "    except NameError:\n",
    "        print(\"Error: `clean_text_initial` function is not defined. Please define it in a cell above.\")\n",
    "        return []\n",
    "    \n",
    "    # 2. Arabize if it contains Latin characters\n",
    "    #    Make it lowercase before arabization as arabize_text expects it\n",
    "    if re.search(r'[a-zA-Z0-9]', processed_text): # Check for Latin or Arabizi numbers\n",
    "        try:\n",
    "            processed_text = arabize_text(processed_text.lower())\n",
    "        except NameError:\n",
    "            print(\"Error: `arabize_text` function (and `ARABIZI_TO_ARABIC_MAP`) is not defined. Please define it.\")\n",
    "            return []\n",
    "\n",
    "    # 3. Normalize Arabic script\n",
    "    try:\n",
    "        processed_text = normalize_arabic_text(processed_text)\n",
    "    except NameError:\n",
    "        print(\"Error: `normalize_arabic_text` function is not defined. Please define it.\")\n",
    "        return []\n",
    "\n",
    "    # 4. Final minimal clean for BPE (remove anything not Arabic or whitespace)\n",
    "    # This ensures only valid characters for an Arabic BPE model are passed.\n",
    "    processed_text = re.sub(r'[^\\p{Arabic}\\s]', '', processed_text, flags=re.UNICODE)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "\n",
    "    if not processed_text:\n",
    "        return []\n",
    "\n",
    "    # 5. BPE Tokenize\n",
    "    bpe_tokens = tokenizer.encode(processed_text).tokens\n",
    "    return bpe_tokens\n",
    "\n",
    "\n",
    "def display_arabic(text: str) -> str:\n",
    "    \"\"\"Helper function to display Arabic text correctly in RTL terminals/outputs.\"\"\"\n",
    "    if RTL_DISPLAY_AVAILABLE:\n",
    "        # return get_display(arabic_reshaper.reshape(text))\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- Main Interaction Loop ---\n",
    "if models_loaded_successfully:\n",
    "    print(\"\\n--- Darija Word Similarity Finder ---\")\n",
    "    print(\"Type a word (Darija in Arabic/Latin script, or French) and press Enter.\")\n",
    "    print(\"Type 'exit' or 'quit' to close.\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nEnter word: \").strip()\n",
    "\n",
    "            if not user_input:\n",
    "                continue\n",
    "            if user_input.lower() in ['exit', 'quit']:\n",
    "                print(\"Exiting...\")\n",
    "                break\n",
    "\n",
    "            print(f\"Original input: '{user_input}'\")\n",
    "\n",
    "            # Preprocess the input word to get BPE tokens\n",
    "            input_bpe_tokens = preprocess_input_word_for_similarity(user_input, bpe_tokenizer)\n",
    "            \n",
    "            if not input_bpe_tokens:\n",
    "                print(\"Input word became empty after preprocessing or resulted in no BPE tokens.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processed BPE tokens: {', '.join([display_arabic(t) for t in input_bpe_tokens])}\")\n",
    "\n",
    "            # Filter out BPE tokens not in Word2Vec vocabulary and [UNK]\n",
    "            # Get the UNK token string from the BPE tokenizer\n",
    "            unk_token_string = bpe_tokenizer.model.unk_token\n",
    "            \n",
    "            valid_bpe_tokens_for_model = [\n",
    "                token for token in input_bpe_tokens \n",
    "                if token in word2vec_model.wv and token != unk_token_string\n",
    "            ]\n",
    "\n",
    "            if not valid_bpe_tokens_for_model:\n",
    "                print(f\"None of the BPE tokens ({', '.join([display_arabic(t) for t in input_bpe_tokens])}) are in the Word2Vec model's vocabulary or they are all [UNK].\")\n",
    "                print(\"This might happen if the word is very rare, out-of-domain, or results only in [UNK] tokens.\")\n",
    "                continue\n",
    "            \n",
    "            if len(valid_bpe_tokens_for_model) < len(input_bpe_tokens):\n",
    "                print(f\"Note: Some BPE sub-tokens were Out-Of-Vocabulary or [UNK] and were excluded from similarity search.\")\n",
    "                print(f\"Using valid BPE tokens for similarity: {', '.join([display_arabic(t) for t in valid_bpe_tokens_for_model])}\")\n",
    "\n",
    "\n",
    "            # Get similar tokens from Word2Vec model\n",
    "            similar_items = word2vec_model.wv.most_similar(positive=valid_bpe_tokens_for_model, topn=5)\n",
    "\n",
    "            print(f\"\\nTop 5 most similar BPE tokens to '{display_arabic(' '.join(valid_bpe_tokens_for_model))}':\")\n",
    "            if similar_items:\n",
    "                for i, (token, score) in enumerate(similar_items):\n",
    "                    print(f\"{i+1}. {display_arabic(token):<15} (Score: {score:.4f})\")\n",
    "            else:\n",
    "                print(\"No similar tokens found.\")\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: One of the processed BPE tokens '{e}' was not found in the model's vocabulary.\")\n",
    "        except NameError as e:\n",
    "            print(f\"Error: A preprocessing function might be missing. {e}. Please define it in a cell above.\")\n",
    "            break # Stop the loop if core functions are missing\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # For detailed debugging during development\n",
    "\n",
    "else:\n",
    "    print(\"\\nCannot start similarity finder: Word2Vec model or BPE tokenizer not loaded, or core preprocessing functions are missing.\")\n",
    "    print(\"Please ensure the paths to models are correct and preprocessing functions are defined in cells above.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
