{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re \n",
    "from gensim.models import Word2Vec\n",
    "from tokenizers import Tokenizer\n",
    "import unicodedata\n",
    "\n",
    "try:\n",
    "    import arabic_reshaper\n",
    "    from bidi.algorithm import get_display\n",
    "    RTL_DISPLAY_AVAILABLE = True\n",
    "    print(\"arabic_reshaper and python-bidi found. RTL display enabled.\")\n",
    "except ImportError:\n",
    "    RTL_DISPLAY_AVAILABLE = False\n",
    "    print(\"Warning: arabic_reshaper or python-bidi not found. RTL display might not be optimal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "WORD2VEC_MODEL_PATH = 'darija_word2vec_bpe_sg_ns.model' \n",
    "\n",
    "BPE_TOKENIZER_PATH = 'processed_darija_v2/darija_bpe_tokenizer.json'      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARABIZI_TO_ARABIC_MAP = {\n",
    "    \n",
    "    '2': 'ء', '3': 'ع', '4': 'غ', '6': 'ط', '8': 'ق', \n",
    "    '7': 'ح', '5': 'خ', '9': 'ق', \n",
    "                                \n",
    "    \n",
    "    'ch': 'ش', 'sh': 'ش', 'kh': 'خ', 'gh': 'غ',\n",
    "    'th': 'ث', 'dh': 'ذ', 'ou': 'و', 'oo': 'و',\n",
    "    \n",
    "    'a': 'ا', 'b': 'ب', 'c': 'س', \n",
    "    'd': 'د', 'e': 'ي', \n",
    "    'f': 'ف', 'g': 'ڭ', \n",
    "    'h': 'ه', 'i': 'ي', 'j': 'ج', 'k': 'ك', 'l': 'ل', 'm': 'م',\n",
    "    'n': 'ن', 'o': 'و', 'p': 'ب', \n",
    "    'q': 'ق', 'r': 'ر', 's': 'س',\n",
    "    't': 'ت', 'u': 'و', 'v': 'ف', \n",
    "    'w': 'و', 'x': 'كس', 'y': 'ي', 'z': 'ز',\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_initial(text: str) -> str:\n",
    "    \"\"\"Performs initial cleaning of the text.\"\"\"\n",
    "    \n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\[\\[File:[^\\]]*\\]\\]', ' ', text)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'\\[\\[(?:[^\\]]*:|[^\\]]*\\|[^\\]]*)\\]\\]', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\[\\[([^\\]]{20,})\\]\\]', ' ', text) \n",
    "    \n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\b\\d+px\\b', ' ', text)\n",
    "    \n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabize_text(text: str) -> str:\n",
    "    \"\"\"Converts Arabizi (Latin script Darija with numbers) to Arabic script.\"\"\"\n",
    "    text = text.lower() \n",
    "\n",
    "    \n",
    "    sorted_keys = sorted(ARABIZI_TO_ARABIC_MAP.keys(), key=len, reverse=True)\n",
    "\n",
    "    for key in sorted_keys:\n",
    "        text = text.replace(key, ARABIZI_TO_ARABIC_MAP[key])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic_text(text: str) -> str:\n",
    "    \"\"\"Normalizes Arabic script.\"\"\"\n",
    "    \n",
    "    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n",
    "    \n",
    "    text = text.replace('\\u0640', '')\n",
    "    \n",
    "    \n",
    "    text = text.replace('أ', 'ا').replace('إ', 'ا').replace('آ', 'ا').replace('ٱ', 'ا')\n",
    "    \n",
    "    \n",
    "    text = text.replace('ة', 'ه')  \n",
    "    text = text.replace('ى', 'ي')  \n",
    "    \n",
    "    \n",
    "    text = text.replace('گ', 'ك')  \n",
    "    text = text.replace('ڭ', 'ك')  \n",
    "                                  \n",
    "    text = text.replace('چ', 'ش')  \n",
    "    text = text.replace('پ', 'ب')  \n",
    "    text = text.replace('ڤ', 'ف')  \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word2vec_model = None\n",
    "bpe_tokenizer = None\n",
    "models_loaded_successfully = False\n",
    "\n",
    "try:\n",
    "    if os.path.exists(WORD2VEC_MODEL_PATH):\n",
    "        print(f\"Loading Word2Vec model from: {WORD2VEC_MODEL_PATH}\")\n",
    "        word2vec_model = Word2Vec.load(WORD2VEC_MODEL_PATH)\n",
    "        print(\"Word2Vec model loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Error: Word2Vec model file not found at {WORD2VEC_MODEL_PATH}\")\n",
    "\n",
    "    if os.path.exists(BPE_TOKENIZER_PATH):\n",
    "        print(f\"Loading BPE tokenizer from: {BPE_TOKENIZER_PATH}\")\n",
    "        bpe_tokenizer = Tokenizer.from_file(BPE_TOKENIZER_PATH)\n",
    "        print(\"BPE tokenizer loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Error: BPE tokenizer file not found at {BPE_TOKENIZER_PATH}\")\n",
    "\n",
    "    if word2vec_model and bpe_tokenizer:\n",
    "        models_loaded_successfully = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model loading: {e}\")\n",
    "\n",
    "\n",
    "def preprocess_input_word_for_similarity(text: str, tokenizer: Tokenizer) -> list[str]:\n",
    "    \"\"\"\n",
    "    Applies necessary preprocessing to a single input word/phrase\n",
    "    and returns its BPE tokens.\n",
    "    Assumes clean_text_initial, arabize_text, normalize_arabic_text are defined.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        processed_text = clean_text_initial(text)\n",
    "    except NameError:\n",
    "        print(\"Error: `clean_text_initial` function is not defined. Please define it in a cell above.\")\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    \n",
    "    if re.search(r'[a-zA-Z0-9]', processed_text): \n",
    "        try:\n",
    "            processed_text = arabize_text(processed_text.lower())\n",
    "        except NameError:\n",
    "            print(\"Error: `arabize_text` function (and `ARABIZI_TO_ARABIC_MAP`) is not defined. Please define it.\")\n",
    "            return []\n",
    "\n",
    "    \n",
    "    try:\n",
    "        processed_text = normalize_arabic_text(processed_text)\n",
    "    except NameError:\n",
    "        print(\"Error: `normalize_arabic_text` function is not defined. Please define it.\")\n",
    "        return []\n",
    "\n",
    "    \n",
    "    \n",
    "    processed_text = re.sub(r'[^\\p{Arabic}\\s]', '', processed_text, flags=re.UNICODE)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "\n",
    "    if not processed_text:\n",
    "        return []\n",
    "\n",
    "    \n",
    "    bpe_tokens = tokenizer.encode(processed_text).tokens\n",
    "    return bpe_tokens\n",
    "\n",
    "\n",
    "def display_arabic(text: str) -> str:\n",
    "    \"\"\"Helper function to display Arabic text correctly in RTL terminals/outputs.\"\"\"\n",
    "    if RTL_DISPLAY_AVAILABLE:\n",
    "        \n",
    "        pass\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "if models_loaded_successfully:\n",
    "    print(\"\\n--- Darija Word Similarity Finder ---\")\n",
    "    print(\"Type a word (Darija in Arabic/Latin script, or French) and press Enter.\")\n",
    "    print(\"Type 'exit' or 'quit' to close.\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nEnter word: \").strip()\n",
    "\n",
    "            if not user_input:\n",
    "                continue\n",
    "            if user_input.lower() in ['exit', 'quit']:\n",
    "                print(\"Exiting...\")\n",
    "                break\n",
    "\n",
    "            print(f\"Original input: '{user_input}'\")\n",
    "\n",
    "            \n",
    "            input_bpe_tokens = preprocess_input_word_for_similarity(user_input, bpe_tokenizer)\n",
    "            \n",
    "            if not input_bpe_tokens:\n",
    "                print(\"Input word became empty after preprocessing or resulted in no BPE tokens.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processed BPE tokens: {', '.join([display_arabic(t) for t in input_bpe_tokens])}\")\n",
    "\n",
    "            \n",
    "            \n",
    "            unk_token_string = bpe_tokenizer.model.unk_token\n",
    "            \n",
    "            valid_bpe_tokens_for_model = [\n",
    "                token for token in input_bpe_tokens \n",
    "                if token in word2vec_model.wv and token != unk_token_string\n",
    "            ]\n",
    "\n",
    "            if not valid_bpe_tokens_for_model:\n",
    "                print(f\"None of the BPE tokens ({', '.join([display_arabic(t) for t in input_bpe_tokens])}) are in the Word2Vec model's vocabulary or they are all [UNK].\")\n",
    "                print(\"This might happen if the word is very rare, out-of-domain, or results only in [UNK] tokens.\")\n",
    "                continue\n",
    "            \n",
    "            if len(valid_bpe_tokens_for_model) < len(input_bpe_tokens):\n",
    "                print(f\"Note: Some BPE sub-tokens were Out-Of-Vocabulary or [UNK] and were excluded from similarity search.\")\n",
    "                print(f\"Using valid BPE tokens for similarity: {', '.join([display_arabic(t) for t in valid_bpe_tokens_for_model])}\")\n",
    "\n",
    "\n",
    "            \n",
    "            similar_items = word2vec_model.wv.most_similar(positive=valid_bpe_tokens_for_model, topn=5)\n",
    "\n",
    "            print(f\"\\nTop 5 most similar BPE tokens to '{display_arabic(' '.join(valid_bpe_tokens_for_model))}':\")\n",
    "            if similar_items:\n",
    "                for i, (token, score) in enumerate(similar_items):\n",
    "                    print(f\"{i+1}. {display_arabic(token):<15} (Score: {score:.4f})\")\n",
    "            else:\n",
    "                print(\"No similar tokens found.\")\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: One of the processed BPE tokens '{e}' was not found in the model's vocabulary.\")\n",
    "        except NameError as e:\n",
    "            print(f\"Error: A preprocessing function might be missing. {e}. Please define it in a cell above.\")\n",
    "            break \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() \n",
    "\n",
    "else:\n",
    "    print(\"\\nCannot start similarity finder: Word2Vec model or BPE tokenizer not loaded, or core preprocessing functions are missing.\")\n",
    "    print(\"Please ensure the paths to models are correct and preprocessing functions are defined in cells above.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
